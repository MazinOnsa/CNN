{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dreamer4EVA/CNN/blob/master/Copy_of_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q",
        "colab_type": "code",
        "outputId": "aa122ad0-64c0-4d22-8ac6-a967afc7b072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.4.2'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.17.5)\n",
            "Requirement already satisfied: torchvision==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.17.5)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.3.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (6.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.12.0)\n",
            "Collecting Pillow-SIMD\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/19/b7043190f481abb94dcdd1e69c4432432aaa73455cf1128eae39b8eb2518/Pillow-SIMD-6.0.0.post0.tar.gz (621kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 2.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow-SIMD\n",
            "  Building wheel for Pillow-SIMD (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow-SIMD: filename=Pillow_SIMD-6.0.0.post0-cp36-cp36m-linux_x86_64.whl size=1062884 sha256=09f5e6dc61bb9d0200796bbf366df820f47c0c2472f59daed228242ad2bebfdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/60/65/cc9afa345ccbf10a34cc208266b992941a8608010b592f43d1\n",
            "Successfully built Pillow-SIMD\n",
            "Installing collected packages: Pillow-SIMD\n",
            "Successfully installed Pillow-SIMD-6.0.0.post0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokFOdD1dJEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh",
        "colab_type": "text"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5PkYfqfK_SA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 102 # 101 + 1: There is am extra Background class that should be removed \n",
        "\n",
        "BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 1e-3            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUDdw4j2H0Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "valid_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGzEKT01A0g8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuauWqRZO-af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "class Caltech(VisionDataset):\n",
        "    def __init__(self, root, split='train', transform=None, target_transform=None):\n",
        "        super(Caltech, self).__init__(root, transform=transform, target_transform=target_transform)\n",
        "\n",
        "        self.split = split # This defines the split you are going to use\n",
        "                           # (split files are called 'train.txt' and 'test.txt')\n",
        "        # Open file in read only mode and read all lines\n",
        "        file = open(self.split, \"r\")\n",
        "        lines = file.readlines()\n",
        "\n",
        "        # identify BACKGROUND class \n",
        "        self.elements = [i for i in lines if not i.startswith('BACKGROUND_Google')]\n",
        "\n",
        "        # Delete BACKGROUND_Google class from dataset labels\n",
        "        self.classes = sorted(os.listdir(os.path.join(self.root, \"\")))\n",
        "        self.classes.remove(\"BACKGROUND_Google\")     \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = pil_loader(os.path.join(self.root, self.elements[index].rstrip()))\n",
        "        target = self.classes.index(self.elements[index].rstrip().split('/')[0])\n",
        "        image, label = img, target  # Provide a way to access image and label via index\n",
        "                           # Image should be a PIL Image\n",
        "                           # label can be int\n",
        "        # Applies preprocessing when accessing the image\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        length =  len(self.elements) # Provide a way to get the length (number of elements) of the dataset\n",
        "        return length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuJUzBIi1H83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "def get_train_valid_loader(data_dir,\n",
        "                           batch_size,\n",
        "                           augment,\n",
        "                           random_seed,\n",
        "                           valid_size=0.1,\n",
        "                           shuffle=True,\n",
        "                           show_sample=False,\n",
        "                           num_workers=4,\n",
        "                           pin_memory=False):\n",
        "\n",
        "\n",
        "    # load the dataset\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=train_transform,\n",
        "    )\n",
        "\n",
        "    valid_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=valid_transform,\n",
        "    )\n",
        "\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    return (train_loader, valid_loader)\n",
        "\n",
        "\n",
        "def get_test_loader(data_dir,\n",
        "                    batch_size,\n",
        "                    shuffle=True,\n",
        "                    num_workers=4,\n",
        "                    pin_memory=False):\n",
        "\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    )\n",
        "\n",
        "    # define transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=False,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "\n",
        "    return data_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfVq_uDHLbsR",
        "colab_type": "code",
        "outputId": "961d38a3-2fe4-417c-d7f7-eb1a463f1141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./Homework2-Caltech101'):\n",
        "  !git clone https://github.com/MachineLearning2020/Homework2-Caltech101.git\n",
        "\n",
        "DATA_DIR = 'Homework2-Caltech101/101_ObjectCategories'\n",
        "SPLIT_TRAIN = 'Homework2-Caltech101/train.txt'\n",
        "SPLIT_TEST = 'Homework2-Caltech101/test.txt'\n",
        "\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = Caltech(DATA_DIR, split = SPLIT_TRAIN, transform=train_transform) #torchvision.datasets.ImageFolder(DATA_DIR, transform=train_transform)\n",
        "test_dataset = Caltech(DATA_DIR, split = SPLIT_TEST, transform=eval_transform) #torchvision.datasets.ImageFolder(DATA_DIR, transform=eval_transform)\n",
        "\n",
        "#train_indexes = [idx for idx in range(len(train_dataset)) if idx % 5] #len(train_dataset) gives number of images in the dataset\n",
        "#test_indexes = [idx for idx in range(len(test_dataset)) if not idx % 5]\n",
        "\n",
        "#train_dataset = Subset(train_dataset, train_indexes)\n",
        "#test_dataset = Subset(test_dataset, test_indexes)\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Homework2-Caltech101'...\n",
            "remote: Enumerating objects: 9256, done.\u001b[K\n",
            "remote: Total 9256 (delta 0), reused 0 (delta 0), pack-reused 9256\u001b[K\n",
            "Receiving objects: 100% (9256/9256), 129.48 MiB | 11.19 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "Checking out files: 100% (9149/9149), done.\n",
            "Train Dataset: 5784\n",
            "Test Dataset: 2893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEDQ7Z21ldN",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VriRw8SI1nle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvltjoRQBwRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mazin\n",
        "\n",
        "#invistigate dataset\n",
        "#train_dataset.train_labels.binscount()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZ1t5Qs2z4j",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exHUjtXa22DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = alexnet() # Loading AlexNet model\n",
        "net.classifier[6] = nn.Linear(4096, NUM_CLASSES) # nn.Linear in pytorch is a fully connected layer\n",
        "                                                 # The convolutional layer is nn.Conv2d\n",
        "\n",
        "# We just changed the last layer of AlexNet with a new fully connected layer with 101 outputs\n",
        "# It is mandatory to study torchvision.models.alexnet source code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjq00G94tSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ",
        "colab_type": "text"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcoQ5fD49yT_",
        "colab_type": "code",
        "outputId": "97259100-2e10-4fbf-c7a1-11d490e0f917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "total_loss = 0\n",
        "current_step = 0\n",
        "\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "batch_size_list = [10, 100, 1000]\n",
        "LR_list = [.1, .01, .001, .0001]\n",
        "for BATCH_SIZE in batch_size_list:\n",
        "    for LR in LR_list:\n",
        "        #net = net\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "        optimizer = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "        for epoch in range(1):\n",
        "            total_loss = 0\n",
        "            print('\\n Starting epoch {}/{}, LR = {}, batch size= {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr(), BATCH_SIZE))\n",
        "            for images, labels in train_dataloader:\n",
        "                images = images.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "                net.train() \n",
        "                optimizer.zero_grad() \n",
        "                outputs = net(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                if current_step % LOG_FREQUENCY == 0:\n",
        "                    print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "                loss.backward()  # backward pass: computes gradients\n",
        "                optimizer.step() # update weights based on accumulated gradients\n",
        "                current_step += 1\n",
        "                total_loss += loss.item() * BATCH_SIZE\n",
        "                print('Loss', total_loss)\n",
        "                \n",
        "                scheduler.step() "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Starting epoch 1/30, LR = [0.1], batch size= 10\n",
            "Step 0, Loss 4.616336822509766\n",
            "Loss 46.163368225097656\n",
            "Loss 92.2629451751709\n",
            "Loss 138.39783668518066\n",
            "Loss 184.52869415283203\n",
            "Loss 230.5290699005127\n",
            "Loss 276.22724056243896\n",
            "Loss 321.6485261917114\n",
            "Loss 367.3907423019409\n",
            "Loss 413.1631135940552\n",
            "Loss 459.5051622390747\n",
            "Step 10, Loss 4.537039756774902\n",
            "Loss 504.87555980682373\n",
            "Loss 546.7817306518555\n",
            "Loss 586.1415481567383\n",
            "Loss 630.4885482788086\n",
            "Loss 675.4406070709229\n",
            "Loss 719.6902370452881\n",
            "Loss 762.6440191268921\n",
            "Loss 808.1795406341553\n",
            "Loss 851.9920063018799\n",
            "Loss 897.2073984146118\n",
            "Step 20, Loss 4.598769187927246\n",
            "Loss 943.1950902938843\n",
            "Loss 989.5708465576172\n",
            "Loss 1033.6625003814697\n",
            "Loss 1079.4048690795898\n",
            "Loss 1123.5462141036987\n",
            "Loss 1163.8133668899536\n",
            "Loss 1208.4103775024414\n",
            "Loss 1249.5031023025513\n",
            "Loss 1295.0181913375854\n",
            "Loss 1338.3786296844482\n",
            "Step 30, Loss 4.465550899505615\n",
            "Loss 1383.0341386795044\n",
            "Loss 1428.1380033493042\n",
            "Loss 1473.4617710113525\n",
            "Loss 1516.9126749038696\n",
            "Loss 1560.4451084136963\n",
            "Loss 1604.5086860656738\n",
            "Loss 1646.344861984253\n",
            "Loss 1690.800576210022\n",
            "Loss 1734.7596263885498\n",
            "Loss 1776.7312717437744\n",
            "Step 40, Loss 4.301022529602051\n",
            "Loss 1819.741497039795\n",
            "Loss 1864.6512413024902\n",
            "Loss 1907.0388793945312\n",
            "Loss 1950.6017065048218\n",
            "Loss 1995.231704711914\n",
            "Loss 2037.2140645980835\n",
            "Loss 2078.3050680160522\n",
            "Loss 2123.5064935684204\n",
            "Loss 2165.5858421325684\n",
            "Loss 2208.8813638687134\n",
            "Step 50, Loss 4.3665361404418945\n",
            "Loss 2252.5467252731323\n",
            "Loss 2297.088704109192\n",
            "Loss 2338.2881212234497\n",
            "Loss 2379.8734188079834\n",
            "Loss 2422.5624084472656\n",
            "Loss 2466.7781257629395\n",
            "Loss 2512.120099067688\n",
            "Loss 2556.3641691207886\n",
            "Loss 2598.8801431655884\n",
            "Loss 2643.804898262024\n",
            "Step 60, Loss 4.229870796203613\n",
            "Loss 2686.10360622406\n",
            "Loss 2732.173810005188\n",
            "Loss 2774.811854362488\n",
            "Loss 2819.24364566803\n",
            "Loss 2862.7235555648804\n",
            "Loss 2905.820565223694\n",
            "Loss 2949.534921646118\n",
            "Loss 2989.8499155044556\n",
            "Loss 3032.255516052246\n",
            "Loss 3075.558819770813\n",
            "Step 70, Loss 4.608691215515137\n",
            "Loss 3121.6457319259644\n",
            "Loss 3166.9516944885254\n",
            "Loss 3213.84783744812\n",
            "Loss 3259.30184841156\n",
            "Loss 3303.2236528396606\n",
            "Loss 3347.3147010803223\n",
            "Loss 3390.706262588501\n",
            "Loss 3433.6254835128784\n",
            "Loss 3477.4798250198364\n",
            "Loss 3518.9708614349365\n",
            "Step 80, Loss 4.389975547790527\n",
            "Loss 3562.870616912842\n",
            "Loss 3607.2860431671143\n",
            "Loss 3651.4836406707764\n",
            "Loss 3694.0333366394043\n",
            "Loss 3739.3668699264526\n",
            "Loss 3783.4194231033325\n",
            "Loss 3826.985445022583\n",
            "Loss 3872.2134017944336\n",
            "Loss 3916.7766618728638\n",
            "Loss 3961.2346982955933\n",
            "Step 90, Loss 4.419950008392334\n",
            "Loss 4005.4341983795166\n",
            "Loss 4049.3309593200684\n",
            "Loss 4094.196834564209\n",
            "Loss 4137.527232170105\n",
            "Loss 4181.669483184814\n",
            "Loss 4219.353451728821\n",
            "Loss 4260.916800498962\n",
            "Loss 4305.299673080444\n",
            "Loss 4350.545878410339\n",
            "Loss 4393.730320930481\n",
            "Step 100, Loss 4.352435111999512\n",
            "Loss 4437.254672050476\n",
            "Loss 4479.229035377502\n",
            "Loss 4524.851927757263\n",
            "Loss 4570.974140167236\n",
            "Loss 4616.044383049011\n",
            "Loss 4662.359466552734\n",
            "Loss 4708.604621887207\n",
            "Loss 4752.159042358398\n",
            "Loss 4798.649182319641\n",
            "Loss 4840.314087867737\n",
            "Step 110, Loss 4.475222587585449\n",
            "Loss 4885.066313743591\n",
            "Loss 4927.673363685608\n",
            "Loss 4970.371718406677\n",
            "Loss 5014.48992729187\n",
            "Loss 5059.204292297363\n",
            "Loss 5106.827182769775\n",
            "Loss 5151.383495330811\n",
            "Loss 5197.058205604553\n",
            "Loss 5243.556036949158\n",
            "Loss 5289.7028160095215\n",
            "Step 120, Loss 4.162688255310059\n",
            "Loss 5331.329698562622\n",
            "Loss 5377.833619117737\n",
            "Loss 5421.187982559204\n",
            "Loss 5468.071894645691\n",
            "Loss 5513.118000030518\n",
            "Loss 5551.541213989258\n",
            "Loss 5597.169327735901\n",
            "Loss 5639.746623039246\n",
            "Loss 5683.924593925476\n",
            "Loss 5727.569532394409\n",
            "Step 130, Loss 4.318373203277588\n",
            "Loss 5770.753264427185\n",
            "Loss 5812.653470039368\n",
            "Loss 5854.3050718307495\n",
            "Loss 5900.126252174377\n",
            "Loss 5944.145112037659\n",
            "Loss 5989.643940925598\n",
            "Loss 6034.613070487976\n",
            "Loss 6080.197806358337\n",
            "Loss 6124.400601387024\n",
            "Loss 6170.097403526306\n",
            "Step 140, Loss 4.465189456939697\n",
            "Loss 6214.749298095703\n",
            "Loss 6255.554599761963\n",
            "Loss 6297.868165969849\n",
            "Loss 6344.467830657959\n",
            "Loss 6386.097378730774\n",
            "Loss 6430.036568641663\n",
            "Loss 6472.389979362488\n",
            "Loss 6514.727039337158\n",
            "Loss 6557.003717422485\n",
            "Loss 6601.578664779663\n",
            "Step 150, Loss 4.248681545257568\n",
            "Loss 6644.065480232239\n",
            "Loss 6688.479228019714\n",
            "Loss 6733.97472858429\n",
            "Loss 6780.585837364197\n",
            "Loss 6824.703254699707\n",
            "Loss 6867.910976409912\n",
            "Loss 6912.272911071777\n",
            "Loss 6954.541082382202\n",
            "Loss 6999.505286216736\n",
            "Loss 7042.600383758545\n",
            "Step 160, Loss 4.4539313316345215\n",
            "Loss 7087.13969707489\n",
            "Loss 7131.38623714447\n",
            "Loss 7172.964949607849\n",
            "Loss 7215.663361549377\n",
            "Loss 7260.332102775574\n",
            "Loss 7306.502594947815\n",
            "Loss 7348.52153301239\n",
            "Loss 7389.807696342468\n",
            "Loss 7429.734637737274\n",
            "Loss 7473.815324306488\n",
            "Step 170, Loss 4.16738224029541\n",
            "Loss 7515.489146709442\n",
            "Loss 7560.459506511688\n",
            "Loss 7603.789823055267\n",
            "Loss 7648.542582988739\n",
            "Loss 7693.393695354462\n",
            "Loss 7735.893023014069\n",
            "Loss 7781.122372150421\n",
            "Loss 7822.394258975983\n",
            "Loss 7868.129823207855\n",
            "Loss 7911.851871013641\n",
            "Step 180, Loss 4.221449851989746\n",
            "Loss 7954.066369533539\n",
            "Loss 7997.85383939743\n",
            "Loss 8040.792877674103\n",
            "Loss 8082.5816559791565\n",
            "Loss 8127.863509654999\n",
            "Loss 8171.489522457123\n",
            "Loss 8216.870806217194\n",
            "Loss 8259.847657680511\n",
            "Loss 8302.100207805634\n",
            "Loss 8348.554933071136\n",
            "Step 190, Loss 4.253872871398926\n",
            "Loss 8391.093661785126\n",
            "Loss 8438.49677324295\n",
            "Loss 8479.655096530914\n",
            "Loss 8527.413666248322\n",
            "Loss 8571.835539340973\n",
            "Loss 8617.24074602127\n",
            "Loss 8661.437222957611\n",
            "Loss 8704.082844257355\n",
            "Loss 8749.72681760788\n",
            "Loss 8793.984920978546\n",
            "Step 200, Loss 4.405770301818848\n",
            "Loss 8838.042623996735\n",
            "Loss 8884.210107326508\n",
            "Loss 8929.26213979721\n",
            "Loss 8969.884746074677\n",
            "Loss 9014.036228656769\n",
            "Loss 9055.35118341446\n",
            "Loss 9100.18646478653\n",
            "Loss 9141.097557544708\n",
            "Loss 9185.050375461578\n",
            "Loss 9229.39915895462\n",
            "Step 210, Loss 4.309254169464111\n",
            "Loss 9272.491700649261\n",
            "Loss 9314.49952840805\n",
            "Loss 9361.910073757172\n",
            "Loss 9406.660778522491\n",
            "Loss 9446.631851196289\n",
            "Loss 9490.897006988525\n",
            "Loss 9536.218538284302\n",
            "Loss 9579.0380859375\n",
            "Loss 9620.253744125366\n",
            "Loss 9667.613916397095\n",
            "Step 220, Loss 4.149750709533691\n",
            "Loss 9709.111423492432\n",
            "Loss 9750.428147315979\n",
            "Loss 9796.084480285645\n",
            "Loss 9843.594546318054\n",
            "Loss 9884.881405830383\n",
            "Loss 9930.663924217224\n",
            "Loss 9976.623883247375\n",
            "Loss 10021.074843406677\n",
            "Loss 10065.68233013153\n",
            "Loss 10109.768137931824\n",
            "Step 230, Loss 4.356653213500977\n",
            "Loss 10153.334670066833\n",
            "Loss 10198.353161811829\n",
            "Loss 10242.581357955933\n",
            "Loss 10287.808361053467\n",
            "Loss 10328.468427658081\n",
            "Loss 10373.601937294006\n",
            "Loss 10418.266577720642\n",
            "Loss 10462.339520454407\n",
            "Loss 10503.919124603271\n",
            "Loss 10549.772915840149\n",
            "Step 240, Loss 4.367542743682861\n",
            "Loss 10593.448343276978\n",
            "Loss 10635.04861831665\n",
            "Loss 10680.27006149292\n",
            "Loss 10723.3109998703\n",
            "Loss 10765.850958824158\n",
            "Loss 10812.141318321228\n",
            "Loss 10854.22116279602\n",
            "Loss 10895.622463226318\n",
            "Loss 10938.264532089233\n",
            "Loss 10978.193984031677\n",
            "Step 250, Loss 4.341017723083496\n",
            "Loss 11021.604161262512\n",
            "Loss 11065.220346450806\n",
            "Loss 11105.584516525269\n",
            "Loss 11151.499881744385\n",
            "Loss 11193.86929988861\n",
            "Loss 11241.218762397766\n",
            "Loss 11281.14384174347\n",
            "Loss 11325.741305351257\n",
            "Loss 11368.387722969055\n",
            "Loss 11413.196954727173\n",
            "Step 260, Loss 4.33605432510376\n",
            "Loss 11456.55749797821\n",
            "Loss 11499.643425941467\n",
            "Loss 11543.520512580872\n",
            "Loss 11585.22988319397\n",
            "Loss 11631.363296508789\n",
            "Loss 11673.458285331726\n",
            "Loss 11719.781746864319\n",
            "Loss 11760.456056594849\n",
            "Loss 11803.103218078613\n",
            "Loss 11848.435564041138\n",
            "Step 270, Loss 4.374277591705322\n",
            "Loss 11892.178339958191\n",
            "Loss 11936.41146659851\n",
            "Loss 11982.075066566467\n",
            "Loss 12024.479613304138\n",
            "Loss 12066.004962921143\n",
            "Loss 12110.022039413452\n",
            "Loss 12154.548177719116\n",
            "Loss 12197.275791168213\n",
            "Loss 12240.465540885925\n",
            "Loss 12285.844626426697\n",
            "Step 280, Loss 4.713845729827881\n",
            "Loss 12332.983083724976\n",
            "Loss 12376.544513702393\n",
            "Loss 12420.804481506348\n",
            "Loss 12466.7094373703\n",
            "Loss 12509.114809036255\n",
            "Loss 12551.648316383362\n",
            "Loss 12596.486639976501\n",
            "Loss 12641.67540550232\n",
            "Loss 12682.401232719421\n",
            "Loss 12721.285464763641\n",
            "Step 290, Loss 4.255924224853516\n",
            "Loss 12763.844707012177\n",
            "Loss 12808.011019229889\n",
            "Loss 12854.471867084503\n",
            "Loss 12899.627468585968\n",
            "Loss 12943.902924060822\n",
            "Loss 12991.307981014252\n",
            "Loss 13032.962148189545\n",
            "Loss 13075.236518383026\n",
            "Loss 13117.419192790985\n",
            "Loss 13164.271833896637\n",
            "Step 300, Loss 4.513092994689941\n",
            "Loss 13209.402763843536\n",
            "Loss 13251.194198131561\n",
            "Loss 13294.384953975677\n",
            "Loss 13342.399861812592\n",
            "Loss 13385.846974849701\n",
            "Loss 13427.745659351349\n",
            "Loss 13473.77135515213\n",
            "Loss 13518.601157665253\n",
            "Loss 13560.735914707184\n",
            "Loss 13605.519063472748\n",
            "Step 310, Loss 4.468173027038574\n",
            "Loss 13650.200793743134\n",
            "Loss 13693.176257610321\n",
            "Loss 13736.016290187836\n",
            "Loss 13783.008682727814\n",
            "Loss 13827.52657175064\n",
            "Loss 13872.792150974274\n",
            "Loss 13918.639223575592\n",
            "Loss 13962.969887256622\n",
            "Loss 14008.787381649017\n",
            "Loss 14054.345352649689\n",
            "Step 320, Loss 4.556567192077637\n",
            "Loss 14099.911024570465\n",
            "Loss 14144.584872722626\n",
            "Loss 14186.933977603912\n",
            "Loss 14230.74154138565\n",
            "Loss 14274.224865436554\n",
            "Loss 14316.919176578522\n",
            "Loss 14362.807824611664\n",
            "Loss 14406.334331035614\n",
            "Loss 14450.94040632248\n",
            "Loss 14494.77374792099\n",
            "Step 330, Loss 4.428465366363525\n",
            "Loss 14539.058401584625\n",
            "Loss 14583.84438753128\n",
            "Loss 14628.51280927658\n",
            "Loss 14672.943012714386\n",
            "Loss 14716.280972957611\n",
            "Loss 14757.615640163422\n",
            "Loss 14803.501651287079\n",
            "Loss 14848.54774236679\n",
            "Loss 14894.68299150467\n",
            "Loss 14938.96024942398\n",
            "Step 340, Loss 4.058592796325684\n",
            "Loss 14979.546177387238\n",
            "Loss 15023.206422328949\n",
            "Loss 15064.834325313568\n",
            "Loss 15109.336392879486\n",
            "Loss 15153.79555940628\n",
            "Loss 15198.166892528534\n",
            "Loss 15239.848592281342\n",
            "Loss 15284.789025783539\n",
            "Loss 15331.634948253632\n",
            "Loss 15375.800879001617\n",
            "Step 350, Loss 4.139869689941406\n",
            "Loss 15417.199575901031\n",
            "Loss 15462.701532840729\n",
            "Loss 15504.937918186188\n",
            "Loss 15546.37811422348\n",
            "Loss 15589.238941669464\n",
            "Loss 15633.121817111969\n",
            "Loss 15677.389390468597\n",
            "Loss 15718.631827831268\n",
            "Loss 15762.353699207306\n",
            "Loss 15805.003736019135\n",
            "Step 360, Loss 4.533590793609619\n",
            "Loss 15850.33964395523\n",
            "Loss 15895.118458271027\n",
            "Loss 15938.26239824295\n",
            "Loss 15984.746477603912\n",
            "Loss 16029.493992328644\n",
            "Loss 16073.88239145279\n",
            "Loss 16118.858816623688\n",
            "Loss 16159.955899715424\n",
            "Loss 16201.055085659027\n",
            "Loss 16246.91862821579\n",
            "Step 370, Loss 4.309905529022217\n",
            "Loss 16290.017683506012\n",
            "Loss 16332.187631130219\n",
            "Loss 16373.531830310822\n",
            "Loss 16413.60780954361\n",
            "Loss 16455.579817295074\n",
            "Loss 16501.43128156662\n",
            "Loss 16544.97201681137\n",
            "Loss 16592.688443660736\n",
            "Loss 16636.598522663116\n",
            "Loss 16682.260382175446\n",
            "Step 380, Loss 4.383251190185547\n",
            "Loss 16726.0928940773\n",
            "Loss 16768.943502902985\n",
            "Loss 16813.901183605194\n",
            "Loss 16857.70530462265\n",
            "Loss 16904.850437641144\n",
            "Loss 16946.82914018631\n",
            "Loss 16991.79785013199\n",
            "Loss 17035.348999500275\n",
            "Loss 17079.00705099106\n",
            "Loss 17124.37076330185\n",
            "Step 390, Loss 4.549875736236572\n",
            "Loss 17169.869520664215\n",
            "Loss 17211.46106481552\n",
            "Loss 17258.14018011093\n",
            "Loss 17302.180898189545\n",
            "Loss 17346.86310529709\n",
            "Loss 17389.354436397552\n",
            "Loss 17432.52849340439\n",
            "Loss 17478.29091310501\n",
            "Loss 17521.88122034073\n",
            "Loss 17560.805869102478\n",
            "Step 400, Loss 4.285843849182129\n",
            "Loss 17603.6643075943\n",
            "Loss 17650.79905986786\n",
            "Loss 17694.59852695465\n",
            "Loss 17736.510763168335\n",
            "Loss 17780.95534324646\n",
            "Loss 17825.101823806763\n",
            "Loss 17868.68814945221\n",
            "Loss 17912.765550613403\n",
            "Loss 17954.49125289917\n",
            "Loss 18000.90554714203\n",
            "Step 410, Loss 4.556280136108398\n",
            "Loss 18046.468348503113\n",
            "Loss 18092.468152046204\n",
            "Loss 18138.584141731262\n",
            "Loss 18181.776599884033\n",
            "Loss 18226.691088676453\n",
            "Loss 18272.329697608948\n",
            "Loss 18316.677021980286\n",
            "Loss 18356.909441947937\n",
            "Loss 18396.773200035095\n",
            "Loss 18440.527567863464\n",
            "Step 420, Loss 4.421538352966309\n",
            "Loss 18484.742951393127\n",
            "Loss 18527.464680671692\n",
            "Loss 18573.28588962555\n",
            "Loss 18617.548050880432\n",
            "Loss 18659.936652183533\n",
            "Loss 18703.906393051147\n",
            "Loss 18746.807765960693\n",
            "Loss 18789.589920043945\n",
            "Loss 18835.42791366577\n",
            "Loss 18877.632751464844\n",
            "Step 430, Loss 4.431679725646973\n",
            "Loss 18921.949548721313\n",
            "Loss 18967.52670288086\n",
            "Loss 19010.13135433197\n",
            "Loss 19052.102241516113\n",
            "Loss 19090.669231414795\n",
            "Loss 19136.49260520935\n",
            "Loss 19178.01279067993\n",
            "Loss 19220.31928062439\n",
            "Loss 19263.63088130951\n",
            "Loss 19309.369730949402\n",
            "Step 440, Loss 4.136260032653809\n",
            "Loss 19350.73233127594\n",
            "Loss 19391.802263259888\n",
            "Loss 19437.41243839264\n",
            "Loss 19478.61400127411\n",
            "Loss 19523.307781219482\n",
            "Loss 19566.02698326111\n",
            "Loss 19610.146508216858\n",
            "Loss 19651.442728042603\n",
            "Loss 19695.310678482056\n",
            "Loss 19741.04190826416\n",
            "Step 450, Loss 4.3447113037109375\n",
            "Loss 19784.48902130127\n",
            "Loss 19827.188448905945\n",
            "Loss 19871.129145622253\n",
            "Loss 19910.760669708252\n",
            "Loss 19952.721796035767\n",
            "Loss 19999.27968978882\n",
            "Loss 20042.507028579712\n",
            "Loss 20085.76033115387\n",
            "Loss 20129.487013816833\n",
            "Loss 20169.32298183441\n",
            "Step 460, Loss 4.368926048278809\n",
            "Loss 20213.0122423172\n",
            "Loss 20256.414465904236\n",
            "Loss 20301.29806995392\n",
            "Loss 20344.366536140442\n",
            "Loss 20387.646594047546\n",
            "Loss 20428.403358459473\n",
            "Loss 20472.333068847656\n",
            "Loss 20516.648359298706\n",
            "Loss 20561.115942001343\n",
            "Loss 20604.13067817688\n",
            "Step 470, Loss 4.268946647644043\n",
            "Loss 20646.82014465332\n",
            "Loss 20691.230096817017\n",
            "Loss 20734.607648849487\n",
            "Loss 20779.14134502411\n",
            "Loss 20823.762593269348\n",
            "Loss 20869.373650550842\n",
            "Loss 20913.945541381836\n",
            "Loss 20959.558839797974\n",
            "Loss 21000.670518875122\n",
            "Loss 21044.027614593506\n",
            "Step 480, Loss 4.515229225158691\n",
            "Loss 21089.179906845093\n",
            "Loss 21132.338166236877\n",
            "Loss 21177.81466960907\n",
            "Loss 21222.576179504395\n",
            "Loss 21267.389516830444\n",
            "Loss 21313.6350440979\n",
            "Loss 21360.3927898407\n",
            "Loss 21407.037391662598\n",
            "Loss 21450.736169815063\n",
            "Loss 21492.813730239868\n",
            "Step 490, Loss 4.486302375793457\n",
            "Loss 21537.676753997803\n",
            "Loss 21583.519225120544\n",
            "Loss 21630.786519050598\n",
            "Loss 21675.70375919342\n",
            "Loss 21720.12361049652\n",
            "Loss 21766.073060035706\n",
            "Loss 21809.821701049805\n",
            "Loss 21855.649976730347\n",
            "Loss 21898.665957450867\n",
            "Loss 21944.487686157227\n",
            "Step 500, Loss 4.5278825759887695\n",
            "Loss 21989.766511917114\n",
            "Loss 22033.003554344177\n",
            "Loss 22079.81273651123\n",
            "Loss 22121.747794151306\n",
            "Loss 22165.830903053284\n",
            "Loss 22207.2065448761\n",
            "Loss 22250.719261169434\n",
            "Loss 22291.20771408081\n",
            "Loss 22338.813619613647\n",
            "Loss 22381.465945243835\n",
            "Step 510, Loss 4.470877170562744\n",
            "Loss 22426.174716949463\n",
            "Loss 22467.300972938538\n",
            "Loss 22511.47093296051\n",
            "Loss 22555.989089012146\n",
            "Loss 22595.794813632965\n",
            "Loss 22637.617070674896\n",
            "Loss 22681.083991527557\n",
            "Loss 22719.032649993896\n",
            "Loss 22763.379592895508\n",
            "Loss 22808.606972694397\n",
            "Step 520, Loss 4.2360639572143555\n",
            "Loss 22850.96761226654\n",
            "Loss 22894.863386154175\n",
            "Loss 22939.145102500916\n",
            "Loss 22981.754145622253\n",
            "Loss 23024.403223991394\n",
            "Loss 23068.4659576416\n",
            "Loss 23108.223884105682\n",
            "Loss 23150.770490169525\n",
            "Loss 23194.266078472137\n",
            "Loss 23238.80716562271\n",
            "Step 530, Loss 4.326286315917969\n",
            "Loss 23282.07002878189\n",
            "Loss 23323.361403942108\n",
            "Loss 23369.696671962738\n",
            "Loss 23412.90513753891\n",
            "Loss 23456.424686908722\n",
            "Loss 23498.496997356415\n",
            "Loss 23543.660180568695\n",
            "Loss 23588.692634105682\n",
            "Loss 23631.243941783905\n",
            "Loss 23676.793892383575\n",
            "Step 540, Loss 4.617463111877441\n",
            "Loss 23722.96852350235\n",
            "Loss 23766.716606616974\n",
            "Loss 23812.022511959076\n",
            "Loss 23855.960710048676\n",
            "Loss 23898.536360263824\n",
            "Loss 23939.362099170685\n",
            "Loss 23984.83933210373\n",
            "Loss 24030.54848432541\n",
            "Loss 24073.832347393036\n",
            "Loss 24118.25462579727\n",
            "Step 550, Loss 4.1588945388793945\n",
            "Loss 24159.843571186066\n",
            "Loss 24200.66947698593\n",
            "Loss 24242.26006269455\n",
            "Loss 24284.226410388947\n",
            "Loss 24327.562391757965\n",
            "Loss 24370.608241558075\n",
            "Loss 24415.58823823929\n",
            "Loss 24458.05068254471\n",
            "Loss 24501.744978427887\n",
            "Loss 24544.422500133514\n",
            "Step 560, Loss 4.297982692718506\n",
            "Loss 24587.4023270607\n",
            "Loss 24628.563010692596\n",
            "Loss 24672.2687125206\n",
            "Loss 24715.951640605927\n",
            "Loss 24759.078404903412\n",
            "Loss 24807.0969080925\n",
            "Loss 24852.91728258133\n",
            "Loss 24896.307003498077\n",
            "Loss 24943.263676166534\n",
            "Loss 24984.706242084503\n",
            "Step 570, Loss 4.480398178100586\n",
            "Loss 25029.51022386551\n",
            "Loss 25073.958489894867\n",
            "Loss 25115.967905521393\n",
            "Loss 25159.597856998444\n",
            "Loss 25205.90086221695\n",
            "Loss 25249.52285528183\n",
            "Loss 25293.717086315155\n",
            "Loss 25337.56518125534\n",
            "\n",
            " Starting epoch 1/30, LR = [0.01], batch size= 10\n",
            "Loss 45.04077911376953\n",
            "Loss 89.37315940856934\n",
            "Step 580, Loss 4.447948932647705\n",
            "Loss 133.8526487350464\n",
            "Loss 177.39819526672363\n",
            "Loss 220.4613161087036\n",
            "Loss 263.4548330307007\n",
            "Loss 304.6631717681885\n",
            "Loss 349.03789043426514\n",
            "Loss 392.3955726623535\n",
            "Loss 435.41876792907715\n",
            "Loss 479.7665500640869\n",
            "Loss 523.2705974578857\n",
            "Step 590, Loss 4.533695697784424\n",
            "Loss 568.60755443573\n",
            "Loss 612.4575424194336\n",
            "Loss 656.8649053573608\n",
            "Loss 697.1808576583862\n",
            "Loss 741.5124368667603\n",
            "Loss 788.4947443008423\n",
            "Loss 829.9071931838989\n",
            "Loss 875.1981782913208\n",
            "Loss 922.279462814331\n",
            "Loss 965.5538368225098\n",
            "Step 600, Loss 4.616595268249512\n",
            "Loss 1011.7197895050049\n",
            "Loss 1055.4415845870972\n",
            "Loss 1098.2435274124146\n",
            "Loss 1138.5336256027222\n",
            "Loss 1178.7944269180298\n",
            "Loss 1220.1572751998901\n",
            "Loss 1262.6142168045044\n",
            "Loss 1302.9316329956055\n",
            "Loss 1343.8068675994873\n",
            "Loss 1383.2641243934631\n",
            "Step 610, Loss 4.428196907043457\n",
            "Loss 1427.5460934638977\n",
            "Loss 1470.3210854530334\n",
            "Loss 1513.8015294075012\n",
            "Loss 1558.5843968391418\n",
            "Loss 1600.4753232002258\n",
            "Loss 1645.0737690925598\n",
            "Loss 1687.4846291542053\n",
            "Loss 1728.9925789833069\n",
            "Loss 1768.6430644989014\n",
            "Loss 1813.1942749023438\n",
            "Step 620, Loss 4.773190975189209\n",
            "Loss 1860.9261846542358\n",
            "Loss 1906.2476348876953\n",
            "Loss 1947.899866104126\n",
            "Loss 1991.0391473770142\n",
            "Loss 2033.5340023040771\n",
            "Loss 2075.250082015991\n",
            "Loss 2117.0489501953125\n",
            "Loss 2164.481544494629\n",
            "Loss 2208.8695526123047\n",
            "Loss 2251.722445487976\n",
            "Step 630, Loss 4.179598808288574\n",
            "Loss 2293.518433570862\n",
            "Loss 2334.314227104187\n",
            "Loss 2380.237913131714\n",
            "Loss 2424.109091758728\n",
            "Loss 2470.117268562317\n",
            "Loss 2513.663020133972\n",
            "Loss 2555.0432682037354\n",
            "Loss 2599.4042015075684\n",
            "Loss 2642.8400325775146\n",
            "Loss 2685.2574920654297\n",
            "Step 640, Loss 4.489580154418945\n",
            "Loss 2730.153293609619\n",
            "Loss 2771.8306255340576\n",
            "Loss 2812.6218128204346\n",
            "Loss 2858.4405660629272\n",
            "Loss 2901.492118835449\n",
            "Loss 2942.169237136841\n",
            "Loss 2983.5626792907715\n",
            "Loss 3021.880660057068\n",
            "Loss 3064.2189979553223\n",
            "Loss 3102.9810786247253\n",
            "Step 650, Loss 4.435561180114746\n",
            "Loss 3147.336690425873\n",
            "Loss 3189.1147446632385\n",
            "Loss 3228.1122994422913\n",
            "Loss 3270.312111377716\n",
            "Loss 3310.9558272361755\n",
            "Loss 3357.2489047050476\n",
            "Loss 3398.6591124534607\n",
            "Loss 3441.947057247162\n",
            "Loss 3488.3011507987976\n",
            "Loss 3531.173927783966\n",
            "Step 660, Loss 4.510130405426025\n",
            "Loss 3576.2752318382263\n",
            "Loss 3623.439781665802\n",
            "Loss 3663.3388137817383\n",
            "Loss 3704.677538871765\n",
            "Loss 3746.7580556869507\n",
            "Loss 3793.5741758346558\n",
            "Loss 3835.266785621643\n",
            "Loss 3878.515682220459\n",
            "Loss 3921.7384815216064\n",
            "Loss 3964.013214111328\n",
            "Step 670, Loss 4.352789878845215\n",
            "Loss 4007.5411128997803\n",
            "Loss 4052.263298034668\n",
            "Loss 4098.606095314026\n",
            "Loss 4139.713683128357\n",
            "Loss 4184.214725494385\n",
            "Loss 4227.655692100525\n",
            "Loss 4272.533369064331\n",
            "Loss 4316.888756752014\n",
            "Loss 4359.7057247161865\n",
            "Loss 4402.739624977112\n",
            "Step 680, Loss 4.542752265930176\n",
            "Loss 4448.167147636414\n",
            "Loss 4493.852138519287\n",
            "Loss 4535.236549377441\n",
            "Loss 4576.322550773621\n",
            "Loss 4619.240441322327\n",
            "Loss 4659.602966308594\n",
            "Loss 4706.250205039978\n",
            "Loss 4752.147660255432\n",
            "Loss 4796.452746391296\n",
            "Loss 4841.888546943665\n",
            "Step 690, Loss 3.950615406036377\n",
            "Loss 4881.394701004028\n",
            "Loss 4925.712370872498\n",
            "Loss 4968.276333808899\n",
            "Loss 5009.568133354187\n",
            "Loss 5054.841961860657\n",
            "Loss 5095.554647445679\n",
            "Loss 5141.385517120361\n",
            "Loss 5184.895095825195\n",
            "Loss 5222.404999732971\n",
            "Loss 5263.6128759384155\n",
            "Step 700, Loss 4.465907096862793\n",
            "Loss 5308.2719469070435\n",
            "Loss 5349.493412971497\n",
            "Loss 5388.301477432251\n",
            "Loss 5431.303133964539\n",
            "Loss 5477.856483459473\n",
            "Loss 5520.857200622559\n",
            "Loss 5567.24579334259\n",
            "Loss 5610.911917686462\n",
            "Loss 5650.293822288513\n",
            "Loss 5693.175835609436\n",
            "Step 710, Loss 4.560144424438477\n",
            "Loss 5738.777279853821\n",
            "Loss 5783.508224487305\n",
            "Loss 5826.2367486953735\n",
            "Loss 5866.893792152405\n",
            "Loss 5908.450064659119\n",
            "Loss 5951.6930294036865\n",
            "Loss 5997.913951873779\n",
            "Loss 6043.137559890747\n",
            "Loss 6084.835920333862\n",
            "Loss 6124.158022403717\n",
            "Step 720, Loss 4.314859867095947\n",
            "Loss 6167.3066210746765\n",
            "Loss 6212.257630825043\n",
            "Loss 6253.743832111359\n",
            "Loss 6297.075188159943\n",
            "Loss 6338.8073563575745\n",
            "Loss 6385.839064121246\n",
            "Loss 6432.02689409256\n",
            "Loss 6479.431798458099\n",
            "Loss 6523.0878710746765\n",
            "Loss 6564.835021495819\n",
            "Step 730, Loss 4.387948036193848\n",
            "Loss 6608.714501857758\n",
            "Loss 6655.40739774704\n",
            "Loss 6697.145993709564\n",
            "Loss 6740.183303356171\n",
            "Loss 6785.0052762031555\n",
            "Loss 6828.209354877472\n",
            "Loss 6872.626206874847\n",
            "Loss 6914.672129154205\n",
            "Loss 6959.602406024933\n",
            "Loss 7003.864810466766\n",
            "Step 740, Loss 4.3925371170043945\n",
            "Loss 7047.79018163681\n",
            "Loss 7091.409122943878\n",
            "Loss 7129.609971046448\n",
            "Loss 7171.091628074646\n",
            "Loss 7214.517507553101\n",
            "Loss 7253.127274513245\n",
            "Loss 7291.564671993256\n",
            "Loss 7336.464660167694\n",
            "Loss 7381.829259395599\n",
            "Loss 7426.732370853424\n",
            "Step 750, Loss 4.437167167663574\n",
            "Loss 7471.10404253006\n",
            "Loss 7518.627836704254\n",
            "Loss 7563.410866260529\n",
            "Loss 7601.175847053528\n",
            "Loss 7643.339958190918\n",
            "Loss 7683.750109672546\n",
            "Loss 7726.038165092468\n",
            "Loss 7769.436674118042\n",
            "Loss 7813.669333457947\n",
            "Loss 7858.591847419739\n",
            "Step 760, Loss 4.6497344970703125\n",
            "Loss 7905.089192390442\n",
            "Loss 7948.913927078247\n",
            "Loss 7993.993949890137\n",
            "Loss 8036.699547767639\n",
            "Loss 8076.4506316185\n",
            "Loss 8119.7105050086975\n",
            "Loss 8161.835367679596\n",
            "Loss 8197.9274892807\n",
            "Loss 8241.372542381287\n",
            "Loss 8284.973549842834\n",
            "Step 770, Loss 4.237729072570801\n",
            "Loss 8327.350840568542\n",
            "Loss 8370.82284450531\n",
            "Loss 8414.53884601593\n",
            "Loss 8454.601941108704\n",
            "Loss 8495.332822799683\n",
            "Loss 8537.71348953247\n",
            "Loss 8579.405946731567\n",
            "Loss 8623.676071166992\n",
            "Loss 8664.341440200806\n",
            "Loss 8711.184468269348\n",
            "Step 780, Loss 4.313729286193848\n",
            "Loss 8754.321761131287\n",
            "Loss 8801.34617805481\n",
            "Loss 8848.309750556946\n",
            "Loss 8892.98761844635\n",
            "Loss 8935.827493667603\n",
            "Loss 8978.358449935913\n",
            "Loss 9022.842106819153\n",
            "Loss 9067.481007575989\n",
            "Loss 9112.134613990784\n",
            "Loss 9157.675819396973\n",
            "Step 790, Loss 4.244832515716553\n",
            "Loss 9200.124144554138\n",
            "Loss 9242.515473365784\n",
            "Loss 9287.11950302124\n",
            "Loss 9326.78263425827\n",
            "Loss 9371.077916622162\n",
            "Loss 9415.798680782318\n",
            "Loss 9461.64690732956\n",
            "Loss 9505.676038265228\n",
            "Loss 9548.318140506744\n",
            "Loss 9589.965336322784\n",
            "Step 800, Loss 4.258562088012695\n",
            "Loss 9632.550957202911\n",
            "Loss 9680.431687831879\n",
            "Loss 9723.543465137482\n",
            "Loss 9760.689949989319\n",
            "Loss 9797.755055427551\n",
            "Loss 9840.794806480408\n",
            "Loss 9882.809410095215\n",
            "Loss 9930.256915092468\n",
            "Loss 9973.479504585266\n",
            "Loss 10016.193299293518\n",
            "Step 810, Loss 4.428862571716309\n",
            "Loss 10060.481925010681\n",
            "Loss 10103.919320106506\n",
            "Loss 10149.38901424408\n",
            "Loss 10190.591263771057\n",
            "Loss 10236.176691055298\n",
            "Loss 10282.196583747864\n",
            "Loss 10324.134459495544\n",
            "Loss 10362.14213848114\n",
            "Loss 10401.02765083313\n",
            "Loss 10440.028569698334\n",
            "Step 820, Loss 4.5213470458984375\n",
            "Loss 10485.242040157318\n",
            "Loss 10527.793624401093\n",
            "Loss 10572.085039615631\n",
            "Loss 10617.266590595245\n",
            "Loss 10665.428035259247\n",
            "Loss 10712.174389362335\n",
            "Loss 10753.22240114212\n",
            "Loss 10797.72402048111\n",
            "Loss 10843.74127626419\n",
            "Loss 10888.420584201813\n",
            "Step 830, Loss 4.337822914123535\n",
            "Loss 10931.798813343048\n",
            "Loss 10974.953553676605\n",
            "Loss 11015.911829471588\n",
            "Loss 11057.602021694183\n",
            "Loss 11101.136057376862\n",
            "Loss 11147.44528055191\n",
            "Loss 11192.043693065643\n",
            "Loss 11235.411741733551\n",
            "Loss 11280.281431674957\n",
            "Loss 11321.718838214874\n",
            "Step 840, Loss 4.568467140197754\n",
            "Loss 11367.403509616852\n",
            "Loss 11410.609967708588\n",
            "Loss 11457.8848528862\n",
            "Loss 11499.853489398956\n",
            "Loss 11543.88935804367\n",
            "Loss 11586.658279895782\n",
            "Loss 11630.316588878632\n",
            "Loss 11671.811797618866\n",
            "Loss 11714.57138299942\n",
            "Loss 11754.97286081314\n",
            "Step 850, Loss 4.475944519042969\n",
            "Loss 11799.73230600357\n",
            "Loss 11843.481438159943\n",
            "Loss 11890.365374088287\n",
            "Loss 11932.351186275482\n",
            "Loss 11978.442499637604\n",
            "Loss 12023.828279972076\n",
            "Loss 12069.045283794403\n",
            "Loss 12110.482242107391\n",
            "Loss 12152.41625547409\n",
            "Loss 12196.019928455353\n",
            "Step 860, Loss 4.2307610511779785\n",
            "Loss 12238.327538967133\n",
            "Loss 12278.793036937714\n",
            "Loss 12322.185418605804\n",
            "Loss 12361.780908107758\n",
            "Loss 12410.083692073822\n",
            "Loss 12454.231231212616\n",
            "Loss 12493.553881645203\n",
            "Loss 12534.16494846344\n",
            "Loss 12575.553002357483\n",
            "Loss 12620.618081092834\n",
            "Step 870, Loss 4.62051248550415\n",
            "Loss 12666.823205947876\n",
            "Loss 12712.990417480469\n",
            "Loss 12751.584248542786\n",
            "Loss 12789.820001125336\n",
            "Loss 12834.52568769455\n",
            "Loss 12877.96752691269\n",
            "Loss 12919.129779338837\n",
            "Loss 12960.849859714508\n",
            "Loss 13003.069303035736\n",
            "Loss 13044.01160955429\n",
            "Step 880, Loss 4.334443092346191\n",
            "Loss 13087.356040477753\n",
            "Loss 13130.828320980072\n",
            "Loss 13174.692409038544\n",
            "Loss 13222.07738161087\n",
            "Loss 13267.566249370575\n",
            "Loss 13310.663635730743\n",
            "Loss 13349.584572315216\n",
            "Loss 13391.270906925201\n",
            "Loss 13432.168290615082\n",
            "Loss 13479.758455753326\n",
            "Step 890, Loss 4.785666465759277\n",
            "Loss 13527.61512041092\n",
            "Loss 13572.648050785065\n",
            "Loss 13616.749641895294\n",
            "Loss 13661.53131723404\n",
            "Loss 13702.441227436066\n",
            "Loss 13742.5586104393\n",
            "Loss 13784.692471027374\n",
            "Loss 13826.713163852692\n",
            "Loss 13869.93367433548\n",
            "Loss 13911.140468120575\n",
            "Step 900, Loss 4.313326835632324\n",
            "Loss 13954.273736476898\n",
            "Loss 13998.0348944664\n",
            "Loss 14042.832944393158\n",
            "Loss 14089.354207515717\n",
            "Loss 14127.264966964722\n",
            "Loss 14170.530095100403\n",
            "Loss 14210.145618915558\n",
            "Loss 14253.86099100113\n",
            "Loss 14299.934813976288\n",
            "Loss 14342.468740940094\n",
            "Step 910, Loss 4.096799373626709\n",
            "Loss 14383.436734676361\n",
            "Loss 14424.62000131607\n",
            "Loss 14466.289150714874\n",
            "Loss 14509.42806482315\n",
            "Loss 14549.939830303192\n",
            "Loss 14592.793791294098\n",
            "Loss 14636.808483600616\n",
            "Loss 14679.782574176788\n",
            "Loss 14724.957325458527\n",
            "Loss 14769.331753253937\n",
            "Step 920, Loss 4.4593024253845215\n",
            "Loss 14813.924777507782\n",
            "Loss 14855.012171268463\n",
            "Loss 14899.956662654877\n",
            "Loss 14945.953242778778\n",
            "Loss 14992.901198863983\n",
            "Loss 15037.725522518158\n",
            "Loss 15080.859324932098\n",
            "Loss 15123.446156978607\n",
            "Loss 15168.898656368256\n",
            "Loss 15207.393934726715\n",
            "Step 930, Loss 4.648746490478516\n",
            "Loss 15253.8813996315\n",
            "Loss 15299.202473163605\n",
            "Loss 15341.087515354156\n",
            "Loss 15384.42076921463\n",
            "Loss 15426.763608455658\n",
            "Loss 15471.144893169403\n",
            "Loss 15513.61622095108\n",
            "Loss 15556.232221126556\n",
            "Loss 15602.948977947235\n",
            "Loss 15643.821380138397\n",
            "Step 940, Loss 4.40950870513916\n",
            "Loss 15687.916467189789\n",
            "Loss 15729.153425693512\n",
            "Loss 15772.348172664642\n",
            "Loss 15818.227336406708\n",
            "Loss 15862.134149074554\n",
            "Loss 15903.981902599335\n",
            "Loss 15949.734308719635\n",
            "Loss 15993.079159259796\n",
            "Loss 16034.736988544464\n",
            "Loss 16080.327570438385\n",
            "Step 950, Loss 4.47841215133667\n",
            "Loss 16125.111691951752\n",
            "Loss 16163.954000473022\n",
            "Loss 16211.101536750793\n",
            "Loss 16255.18232345581\n",
            "Loss 16298.053674697876\n",
            "Loss 16342.442688941956\n",
            "Loss 16386.441249847412\n",
            "Loss 16432.72596359253\n",
            "Loss 16474.64545249939\n",
            "Loss 16520.069284439087\n",
            "Step 960, Loss 4.614331245422363\n",
            "Loss 16566.21259689331\n",
            "Loss 16613.330416679382\n",
            "Loss 16652.960357666016\n",
            "Loss 16698.276824951172\n",
            "Loss 16740.357131958008\n",
            "Loss 16783.094329833984\n",
            "Loss 16828.6101436615\n",
            "Loss 16871.942563056946\n",
            "Loss 16917.25215435028\n",
            "Loss 16961.43047809601\n",
            "Step 970, Loss 4.308238983154297\n",
            "Loss 17004.51286792755\n",
            "Loss 17050.756859779358\n",
            "Loss 17089.08103942871\n",
            "Loss 17131.30015850067\n",
            "Loss 17174.00848865509\n",
            "Loss 17218.572058677673\n",
            "Loss 17260.508580207825\n",
            "Loss 17307.184009552002\n",
            "Loss 17351.294622421265\n",
            "Loss 17392.11082458496\n",
            "Step 980, Loss 4.093142509460449\n",
            "Loss 17433.042249679565\n",
            "Loss 17477.786531448364\n",
            "Loss 17519.729018211365\n",
            "Loss 17562.429132461548\n",
            "Loss 17609.2099237442\n",
            "Loss 17652.119102478027\n",
            "Loss 17696.307125091553\n",
            "Loss 17737.318501472473\n",
            "Loss 17782.29416847229\n",
            "Loss 17824.63876247406\n",
            "Step 990, Loss 4.047232151031494\n",
            "Loss 17865.111083984375\n",
            "Loss 17909.51910495758\n",
            "Loss 17954.318556785583\n",
            "Loss 17998.215684890747\n",
            "Loss 18042.088313102722\n",
            "Loss 18084.24286365509\n",
            "Loss 18124.07133102417\n",
            "Loss 18170.753812789917\n",
            "Loss 18215.902676582336\n",
            "Loss 18261.91680908203\n",
            "Step 1000, Loss 4.451196670532227\n",
            "Loss 18306.428775787354\n",
            "Loss 18349.31818962097\n",
            "Loss 18395.11353969574\n",
            "Loss 18437.6025056839\n",
            "Loss 18481.557955741882\n",
            "Loss 18523.169407844543\n",
            "Loss 18564.63505744934\n",
            "Loss 18608.048372268677\n",
            "Loss 18648.881549835205\n",
            "Loss 18692.601561546326\n",
            "Step 1010, Loss 4.2760748863220215\n",
            "Loss 18735.362310409546\n",
            "Loss 18779.238843917847\n",
            "Loss 18824.567489624023\n",
            "Loss 18868.21711063385\n",
            "Loss 18914.021821022034\n",
            "Loss 18957.20262527466\n",
            "Loss 18997.29815006256\n",
            "Loss 19041.73523902893\n",
            "Loss 19087.115058898926\n",
            "Loss 19133.29797744751\n",
            "Step 1020, Loss 4.045395851135254\n",
            "Loss 19173.751935958862\n",
            "Loss 19216.74777030945\n",
            "Loss 19260.62678337097\n",
            "Loss 19304.644532203674\n",
            "Loss 19352.301392555237\n",
            "Loss 19391.46500825882\n",
            "Loss 19436.206138134003\n",
            "Loss 19480.699632167816\n",
            "Loss 19524.02713060379\n",
            "Loss 19565.663330554962\n",
            "Step 1030, Loss 4.174346446990967\n",
            "Loss 19607.406795024872\n",
            "Loss 19651.27333879471\n",
            "Loss 19695.758554935455\n",
            "Loss 19740.740559101105\n",
            "Loss 19787.4569439888\n",
            "Loss 19830.80821275711\n",
            "Loss 19874.762966632843\n",
            "Loss 19919.755795001984\n",
            "Loss 19963.848087787628\n",
            "Loss 20009.627368450165\n",
            "Step 1040, Loss 4.485766410827637\n",
            "Loss 20054.48503255844\n",
            "Loss 20099.067232608795\n",
            "Loss 20145.917513370514\n",
            "Loss 20190.587589740753\n",
            "Loss 20234.98081922531\n",
            "Loss 20276.572110652924\n",
            "Loss 20316.08029603958\n",
            "Loss 20360.911247730255\n",
            "Loss 20407.883627414703\n",
            "Loss 20448.302538394928\n",
            "Step 1050, Loss 4.376090049743652\n",
            "Loss 20492.063438892365\n",
            "Loss 20532.25957632065\n",
            "Loss 20575.08817434311\n",
            "Loss 20618.754966259003\n",
            "Loss 20658.8192820549\n",
            "Loss 20705.041840076447\n",
            "Loss 20749.95341539383\n",
            "Loss 20787.70658969879\n",
            "Loss 20833.22160243988\n",
            "Loss 20878.481421470642\n",
            "Step 1060, Loss 4.242268085479736\n",
            "Loss 20920.90410232544\n",
            "Loss 20962.54729270935\n",
            "Loss 21007.860407829285\n",
            "Loss 21052.125840187073\n",
            "Loss 21098.426909446716\n",
            "Loss 21139.48649406433\n",
            "Loss 21178.531737327576\n",
            "Loss 21223.651719093323\n",
            "Loss 21265.838952064514\n",
            "Loss 21307.93900489807\n",
            "Step 1070, Loss 4.246223449707031\n",
            "Loss 21350.40123939514\n",
            "Loss 21398.141689300537\n",
            "Loss 21438.923568725586\n",
            "Loss 21481.510181427002\n",
            "Loss 21526.241941452026\n",
            "Loss 21567.59548187256\n",
            "Loss 21608.59381198883\n",
            "Loss 21655.359296798706\n",
            "Loss 21701.353607177734\n",
            "Loss 21747.740201950073\n",
            "Step 1080, Loss 4.01893949508667\n",
            "Loss 21787.92959690094\n",
            "Loss 21831.96635246277\n",
            "Loss 21877.32749938965\n",
            "Loss 21925.154218673706\n",
            "Loss 21966.875710487366\n",
            "Loss 22008.754830360413\n",
            "Loss 22051.13600730896\n",
            "Loss 22095.476055145264\n",
            "Loss 22138.944215774536\n",
            "Loss 22180.761470794678\n",
            "Step 1090, Loss 3.9129738807678223\n",
            "Loss 22219.891209602356\n",
            "Loss 22262.66613960266\n",
            "Loss 22306.586799621582\n",
            "Loss 22351.408381462097\n",
            "Loss 22392.036385536194\n",
            "Loss 22439.526958465576\n",
            "Loss 22483.542103767395\n",
            "Loss 22523.345985412598\n",
            "Loss 22563.23165178299\n",
            "Loss 22610.120890140533\n",
            "Step 1100, Loss 4.6318159103393555\n",
            "Loss 22656.439049243927\n",
            "Loss 22700.241010189056\n",
            "Loss 22744.444859027863\n",
            "Loss 22790.190722942352\n",
            "Loss 22833.57435464859\n",
            "Loss 22872.658746242523\n",
            "Loss 22917.162001132965\n",
            "Loss 22961.609756946564\n",
            "Loss 23006.974275112152\n",
            "Loss 23047.85012960434\n",
            "Step 1110, Loss 4.381811141967773\n",
            "Loss 23091.668241024017\n",
            "Loss 23132.84774541855\n",
            "Loss 23178.50858926773\n",
            "Loss 23223.401367664337\n",
            "Loss 23268.148353099823\n",
            "Loss 23315.568192005157\n",
            "Loss 23355.4998588562\n",
            "Loss 23400.103826522827\n",
            "Loss 23440.085351467133\n",
            "Loss 23482.12958097458\n",
            "Step 1120, Loss 4.525282859802246\n",
            "Loss 23527.3824095726\n",
            "Loss 23572.699253559113\n",
            "Loss 23618.10915708542\n",
            "Loss 23662.17617750168\n",
            "Loss 23707.483179569244\n",
            "Loss 23745.61933040619\n",
            "Loss 23789.62517261505\n",
            "Loss 23827.34661579132\n",
            "Loss 23871.879591941833\n",
            "Loss 23917.085361480713\n",
            "Step 1130, Loss 4.5651702880859375\n",
            "Loss 23962.737064361572\n",
            "Loss 24009.269604682922\n",
            "Loss 24052.500262260437\n",
            "Loss 24096.629252433777\n",
            "Loss 24140.62475681305\n",
            "Loss 24184.683151245117\n",
            "Loss 24230.352001190186\n",
            "Loss 24274.58152770996\n",
            "Loss 24314.57461118698\n",
            "Loss 24355.01662015915\n",
            "Step 1140, Loss 4.445319652557373\n",
            "Loss 24399.469816684723\n",
            "Loss 24447.125408649445\n",
            "Loss 24494.90341424942\n",
            "Loss 24542.45492696762\n",
            "Loss 24587.053592205048\n",
            "Loss 24631.285569667816\n",
            "Loss 24671.80785894394\n",
            "Loss 24714.4939494133\n",
            "Loss 24758.974015712738\n",
            "Loss 24799.51740503311\n",
            "Step 1150, Loss 3.697469711303711\n",
            "Loss 24836.49210214615\n",
            "Loss 24881.71400785446\n",
            "Loss 24920.623915195465\n",
            "Loss 24963.422367572784\n",
            "Loss 25007.71735906601\n",
            "Loss 25049.54162836075\n",
            "\n",
            " Starting epoch 1/30, LR = [0.001], batch size= 10\n",
            "Loss 43.064231872558594\n",
            "Loss 88.27719688415527\n",
            "Loss 131.3710355758667\n",
            "Loss 172.087140083313\n",
            "Step 1160, Loss 4.782182216644287\n",
            "Loss 219.90896224975586\n",
            "Loss 265.91856479644775\n",
            "Loss 313.9429473876953\n",
            "Loss 357.1045446395874\n",
            "Loss 397.3411798477173\n",
            "Loss 440.1933717727661\n",
            "Loss 487.1172761917114\n",
            "Loss 530.779275894165\n",
            "Loss 573.5866212844849\n",
            "Loss 617.3636054992676\n",
            "Step 1170, Loss 4.478325843811035\n",
            "Loss 662.1468639373779\n",
            "Loss 705.7647132873535\n",
            "Loss 747.787356376648\n",
            "Loss 784.3028140068054\n",
            "Loss 827.6754641532898\n",
            "Loss 870.9494185447693\n",
            "Loss 912.8527140617371\n",
            "Loss 954.1270804405212\n",
            "Loss 999.6016097068787\n",
            "Loss 1043.2687830924988\n",
            "Step 1180, Loss 4.306105136871338\n",
            "Loss 1086.3298344612122\n",
            "Loss 1127.9701209068298\n",
            "Loss 1174.486858844757\n",
            "Loss 1215.1369976997375\n",
            "Loss 1260.2790474891663\n",
            "Loss 1303.6007237434387\n",
            "Loss 1350.3931736946106\n",
            "Loss 1392.5497841835022\n",
            "Loss 1438.1536030769348\n",
            "Loss 1481.3150095939636\n",
            "Step 1190, Loss 4.036809921264648\n",
            "Loss 1521.68310880661\n",
            "Loss 1564.9565482139587\n",
            "Loss 1607.3873972892761\n",
            "Loss 1648.906705379486\n",
            "Loss 1692.883665561676\n",
            "Loss 1738.4538340568542\n",
            "Loss 1782.761423587799\n",
            "Loss 1826.1076760292053\n",
            "Loss 1869.9853539466858\n",
            "Loss 1913.3243250846863\n",
            "Step 1200, Loss 3.9893836975097656\n",
            "Loss 1953.218162059784\n",
            "Loss 1995.964949131012\n",
            "Loss 2038.9077830314636\n",
            "Loss 2085.5397248268127\n",
            "Loss 2130.840928554535\n",
            "Loss 2175.218780040741\n",
            "Loss 2215.7716393470764\n",
            "Loss 2257.025578022003\n",
            "Loss 2299.2893052101135\n",
            "Loss 2342.191731929779\n",
            "Step 1210, Loss 4.549291133880615\n",
            "Loss 2387.684643268585\n",
            "Loss 2431.1629462242126\n",
            "Loss 2473.152334690094\n",
            "Loss 2516.0896468162537\n",
            "Loss 2564.534728527069\n",
            "Loss 2607.3130011558533\n",
            "Loss 2650.8431363105774\n",
            "Loss 2694.9798798561096\n",
            "Loss 2739.6064734458923\n",
            "Loss 2782.858669757843\n",
            "Step 1220, Loss 4.587395668029785\n",
            "Loss 2828.732626438141\n",
            "Loss 2871.2697291374207\n",
            "Loss 2915.120851993561\n",
            "Loss 2954.789755344391\n",
            "Loss 2999.805748462677\n",
            "Loss 3043.7542605400085\n",
            "Loss 3089.2706990242004\n",
            "Loss 3132.0756363868713\n",
            "Loss 3176.4703011512756\n",
            "Loss 3219.4919514656067\n",
            "Step 1230, Loss 4.47683048248291\n",
            "Loss 3264.260256290436\n",
            "Loss 3307.381556034088\n",
            "Loss 3353.9448952674866\n",
            "Loss 3399.884750843048\n",
            "Loss 3446.3531470298767\n",
            "Loss 3490.60409784317\n",
            "Loss 3532.626187801361\n",
            "Loss 3574.679114818573\n",
            "Loss 3617.0389819145203\n",
            "Loss 3661.5775179862976\n",
            "Step 1240, Loss 4.755308628082275\n",
            "Loss 3709.1306042671204\n",
            "Loss 3748.6709666252136\n",
            "Loss 3786.2181401252747\n",
            "Loss 3832.304174900055\n",
            "Loss 3874.3988728523254\n",
            "Loss 3917.7612471580505\n",
            "Loss 3958.743312358856\n",
            "Loss 3999.561812877655\n",
            "Loss 4044.02619600296\n",
            "Loss 4087.175772190094\n",
            "Step 1250, Loss 4.57217264175415\n",
            "Loss 4132.8974986076355\n",
            "Loss 4174.5523047447205\n",
            "Loss 4219.503238201141\n",
            "Loss 4259.1666531562805\n",
            "Loss 4301.572105884552\n",
            "Loss 4345.3599190711975\n",
            "Loss 4390.496070384979\n",
            "Loss 4434.050452709198\n",
            "Loss 4477.935883998871\n",
            "Loss 4523.366916179657\n",
            "Step 1260, Loss 4.350825309753418\n",
            "Loss 4566.875169277191\n",
            "Loss 4609.241211414337\n",
            "Loss 4655.229995250702\n",
            "Loss 4697.528517246246\n",
            "Loss 4738.566944599152\n",
            "Loss 4783.538439273834\n",
            "Loss 4828.420417308807\n",
            "Loss 4869.821240901947\n",
            "Loss 4908.695588111877\n",
            "Loss 4949.44703578949\n",
            "Step 1270, Loss 4.230799674987793\n",
            "Loss 4991.755032539368\n",
            "Loss 5039.377951622009\n",
            "Loss 5082.2005224227905\n",
            "Loss 5126.344451904297\n",
            "Loss 5169.18426990509\n",
            "Loss 5213.306264877319\n",
            "Loss 5254.386019706726\n",
            "Loss 5298.253679275513\n",
            "Loss 5344.879465103149\n",
            "Loss 5392.724323272705\n",
            "Step 1280, Loss 4.245945930480957\n",
            "Loss 5435.183782577515\n",
            "Loss 5476.780667304993\n",
            "Loss 5519.153127670288\n",
            "Loss 5564.559030532837\n",
            "Loss 5608.888416290283\n",
            "Loss 5653.56427192688\n",
            "Loss 5697.756361961365\n",
            "Loss 5741.712961196899\n",
            "Loss 5786.390709877014\n",
            "Loss 5832.368431091309\n",
            "Step 1290, Loss 3.9609858989715576\n",
            "Loss 5871.978290081024\n",
            "Loss 5912.895648479462\n",
            "Loss 5953.448574542999\n",
            "Loss 5992.5003933906555\n",
            "Loss 6038.5717606544495\n",
            "Loss 6081.493680477142\n",
            "Loss 6121.440632343292\n",
            "Loss 6162.854363918304\n",
            "Loss 6208.528506755829\n",
            "Loss 6253.375747203827\n",
            "Step 1300, Loss 4.483693599700928\n",
            "Loss 6298.212683200836\n",
            "Loss 6342.0257687568665\n",
            "Loss 6385.799572467804\n",
            "Loss 6424.714112281799\n",
            "Loss 6468.688769340515\n",
            "Loss 6513.836579322815\n",
            "Loss 6561.538014411926\n",
            "Loss 6603.088178634644\n",
            "Loss 6645.4473304748535\n",
            "Loss 6685.191147327423\n",
            "Step 1310, Loss 4.170638561248779\n",
            "Loss 6726.897532939911\n",
            "Loss 6770.640079975128\n",
            "Loss 6816.695621013641\n",
            "Loss 6861.110074520111\n",
            "Loss 6904.976012706757\n",
            "Loss 6948.6169266700745\n",
            "Loss 6993.4311509132385\n",
            "Loss 7035.368587970734\n",
            "Loss 7082.556674480438\n",
            "Loss 7123.536431789398\n",
            "Step 1320, Loss 4.139581203460693\n",
            "Loss 7164.932243824005\n",
            "Loss 7206.197884082794\n",
            "Loss 7248.909161090851\n",
            "Loss 7292.424008846283\n",
            "Loss 7338.178040981293\n",
            "Loss 7380.147202014923\n",
            "Loss 7423.159081935883\n",
            "Loss 7463.6029171943665\n",
            "Loss 7505.623462200165\n",
            "Loss 7551.6134333610535\n",
            "Step 1330, Loss 4.412766456604004\n",
            "Loss 7595.7410979270935\n",
            "Loss 7635.758922100067\n",
            "Loss 7679.293820858002\n",
            "Loss 7721.428253650665\n",
            "Loss 7766.21342420578\n",
            "Loss 7808.677966594696\n",
            "Loss 7853.457033634186\n",
            "Loss 7900.124533176422\n",
            "Loss 7947.822539806366\n",
            "Loss 7989.143378734589\n",
            "Step 1340, Loss 4.768139839172363\n",
            "Loss 8036.824777126312\n",
            "Loss 8080.347940921783\n",
            "Loss 8125.222270488739\n",
            "Loss 8169.811494350433\n",
            "Loss 8212.52475976944\n",
            "Loss 8254.760038852692\n",
            "Loss 8300.285723209381\n",
            "Loss 8337.917563915253\n",
            "Loss 8377.063248157501\n",
            "Loss 8421.833827495575\n",
            "Step 1350, Loss 4.489217281341553\n",
            "Loss 8466.72600030899\n",
            "Loss 8507.060158252716\n",
            "Loss 8546.134269237518\n",
            "Loss 8587.151339054108\n",
            "Loss 8630.124757289886\n",
            "Loss 8673.241531848907\n",
            "Loss 8716.384761333466\n",
            "Loss 8759.686105251312\n",
            "Loss 8805.927293300629\n",
            "Loss 8848.922336101532\n",
            "Step 1360, Loss 4.590104579925537\n",
            "Loss 8894.823381900787\n",
            "Loss 8936.85720205307\n",
            "Loss 8977.563617229462\n",
            "Loss 9023.667919635773\n",
            "Loss 9064.113266468048\n",
            "Loss 9105.745193958282\n",
            "Loss 9150.023658275604\n",
            "Loss 9192.517144680023\n",
            "Loss 9234.988148212433\n",
            "Loss 9278.68647813797\n",
            "Step 1370, Loss 4.714803695678711\n",
            "Loss 9325.834515094757\n",
            "Loss 9368.29775094986\n",
            "Loss 9411.953217983246\n",
            "Loss 9457.058804035187\n",
            "Loss 9503.00017118454\n",
            "Loss 9542.866942882538\n",
            "Loss 9583.292829990387\n",
            "Loss 9626.87887430191\n",
            "Loss 9670.506732463837\n",
            "Loss 9713.826253414154\n",
            "Step 1380, Loss 4.239847660064697\n",
            "Loss 9756.224730014801\n",
            "Loss 9802.241461277008\n",
            "Loss 9847.467682361603\n",
            "Loss 9890.9028840065\n",
            "Loss 9934.529564380646\n",
            "Loss 9973.619678020477\n",
            "Loss 10019.220731258392\n",
            "Loss 10062.196590900421\n",
            "Loss 10103.870303630829\n",
            "Loss 10148.46153974533\n",
            "Step 1390, Loss 4.586880683898926\n",
            "Loss 10194.33034658432\n",
            "Loss 10239.328782558441\n",
            "Loss 10282.860367298126\n",
            "Loss 10323.689978122711\n",
            "Loss 10371.475427150726\n",
            "Loss 10413.799307346344\n",
            "Loss 10455.34585237503\n",
            "Loss 10497.922995090485\n",
            "Loss 10541.14476442337\n",
            "Loss 10580.461356639862\n",
            "Step 1400, Loss 4.3443403244018555\n",
            "Loss 10623.90475988388\n",
            "Loss 10666.964151859283\n",
            "Loss 10711.96073293686\n",
            "Loss 10755.589396953583\n",
            "Loss 10800.129325389862\n",
            "Loss 10842.279183864594\n",
            "Loss 10884.401667118073\n",
            "Loss 10926.447813510895\n",
            "Loss 10967.990930080414\n",
            "Loss 11011.200950145721\n",
            "Step 1410, Loss 4.023258686065674\n",
            "Loss 11051.433537006378\n",
            "Loss 11090.8314371109\n",
            "Loss 11130.276882648468\n",
            "Loss 11172.823894023895\n",
            "Loss 11216.764385700226\n",
            "Loss 11262.98947095871\n",
            "Loss 11307.864644527435\n",
            "Loss 11354.463894367218\n",
            "Loss 11397.805240154266\n",
            "Loss 11442.027313709259\n",
            "Step 1420, Loss 4.3591628074646\n",
            "Loss 11485.618941783905\n",
            "Loss 11529.39978837967\n",
            "Loss 11572.212374210358\n",
            "Loss 11614.759266376495\n",
            "Loss 11656.675584316254\n",
            "Loss 11702.305057048798\n",
            "Loss 11744.743168354034\n",
            "Loss 11787.449605464935\n",
            "Loss 11833.309171199799\n",
            "Loss 11876.238987445831\n",
            "Step 1430, Loss 4.395946979522705\n",
            "Loss 11920.198457241058\n",
            "Loss 11963.224847316742\n",
            "Loss 12005.133974552155\n",
            "Loss 12047.294509410858\n",
            "Loss 12091.647889614105\n",
            "Loss 12134.159185886383\n",
            "Loss 12180.739204883575\n",
            "Loss 12220.861723423004\n",
            "Loss 12263.970291614532\n",
            "Loss 12305.039427280426\n",
            "Step 1440, Loss 4.787613868713379\n",
            "Loss 12352.91556596756\n",
            "Loss 12391.162598133087\n",
            "Loss 12435.860860347748\n",
            "Loss 12475.300099849701\n",
            "Loss 12517.532694339752\n",
            "Loss 12564.733684062958\n",
            "Loss 12605.450899600983\n",
            "Loss 12648.466742038727\n",
            "Loss 12689.28197145462\n",
            "Loss 12734.005630016327\n",
            "Step 1450, Loss 4.446591377258301\n",
            "Loss 12778.47154378891\n",
            "Loss 12819.759042263031\n",
            "Loss 12866.05443239212\n",
            "Loss 12913.392608165741\n",
            "Loss 12960.099613666534\n",
            "Loss 13003.08785200119\n",
            "Loss 13049.081451892853\n",
            "Loss 13093.340284824371\n",
            "Loss 13136.64766550064\n",
            "Loss 13179.957559108734\n",
            "Step 1460, Loss 4.573573112487793\n",
            "Loss 13225.693290233612\n",
            "Loss 13268.08317899704\n",
            "Loss 13316.239092350006\n",
            "Loss 13361.20142698288\n",
            "Loss 13401.933476924896\n",
            "Loss 13445.840170383453\n",
            "Loss 13488.66239786148\n",
            "Loss 13532.246000766754\n",
            "Loss 13576.040546894073\n",
            "Loss 13616.969878673553\n",
            "Step 1470, Loss 3.7273471355438232\n",
            "Loss 13654.243350028992\n",
            "Loss 13700.106797218323\n",
            "Loss 13743.849444389343\n",
            "Loss 13783.690631389618\n",
            "Loss 13827.261703014374\n",
            "Loss 13869.040429592133\n",
            "Loss 13913.199942111969\n",
            "Loss 13956.564075946808\n",
            "Loss 13999.091136455536\n",
            "Loss 14041.810052394867\n",
            "Step 1480, Loss 4.271457195281982\n",
            "Loss 14084.524624347687\n",
            "Loss 14124.88605260849\n",
            "Loss 14168.190591335297\n",
            "Loss 14206.874966621399\n",
            "Loss 14248.590335845947\n",
            "Loss 14292.99949169159\n",
            "Loss 14336.273074150085\n",
            "Loss 14377.537212371826\n",
            "Loss 14418.227710723877\n",
            "Loss 14460.036363601685\n",
            "Step 1490, Loss 4.56375789642334\n",
            "Loss 14505.673942565918\n",
            "Loss 14547.622194290161\n",
            "Loss 14590.637049674988\n",
            "Loss 14629.289946556091\n",
            "Loss 14670.483288764954\n",
            "Loss 14713.856391906738\n",
            "Loss 14756.421766281128\n",
            "Loss 14799.320359230042\n",
            "Loss 14842.122192382812\n",
            "Loss 14887.665419578552\n",
            "Step 1500, Loss 4.572413921356201\n",
            "Loss 14933.389558792114\n",
            "Loss 14975.614728927612\n",
            "Loss 15018.47749710083\n",
            "Loss 15057.802257537842\n",
            "Loss 15103.568696975708\n",
            "Loss 15148.59206199646\n",
            "Loss 15193.396496772766\n",
            "Loss 15236.905665397644\n",
            "Loss 15281.36504650116\n",
            "Loss 15325.907506942749\n",
            "Step 1510, Loss 4.533641338348389\n",
            "Loss 15371.243920326233\n",
            "Loss 15411.990599632263\n",
            "Loss 15459.35510635376\n",
            "Loss 15504.65449810028\n",
            "Loss 15549.541025161743\n",
            "Loss 15591.409168243408\n",
            "Loss 15632.226705551147\n",
            "Loss 15679.947652816772\n",
            "Loss 15724.288120269775\n",
            "Loss 15763.616375923157\n",
            "Step 1520, Loss 4.301718235015869\n",
            "Loss 15806.633558273315\n",
            "Loss 15849.17278289795\n",
            "Loss 15893.702292442322\n",
            "Loss 15935.377898216248\n",
            "Loss 15979.793329238892\n",
            "Loss 16020.320281982422\n",
            "Loss 16061.242332458496\n",
            "Loss 16100.323638916016\n",
            "Loss 16142.889213562012\n",
            "Loss 16186.391859054565\n",
            "Step 1530, Loss 4.215052604675293\n",
            "Loss 16228.542385101318\n",
            "Loss 16270.965089797974\n",
            "Loss 16316.438331604004\n",
            "Loss 16359.9365234375\n",
            "Loss 16402.168741226196\n",
            "Loss 16446.651191711426\n",
            "Loss 16488.996968269348\n",
            "Loss 16533.536157608032\n",
            "Loss 16577.861728668213\n",
            "Loss 16620.600237846375\n",
            "Step 1540, Loss 4.53446102142334\n",
            "Loss 16665.944848060608\n",
            "Loss 16710.858478546143\n",
            "Loss 16755.10729789734\n",
            "Loss 16795.46257019043\n",
            "Loss 16837.25121498108\n",
            "Loss 16882.25230693817\n",
            "Loss 16927.803325653076\n",
            "Loss 16969.221863746643\n",
            "Loss 17010.10666847229\n",
            "Loss 17052.48926639557\n",
            "Step 1550, Loss 3.9339871406555176\n",
            "Loss 17091.829137802124\n",
            "Loss 17135.89089870453\n",
            "Loss 17179.93031024933\n",
            "Loss 17224.969081878662\n",
            "Loss 17267.225008010864\n",
            "Loss 17308.88129711151\n",
            "Loss 17349.705452919006\n",
            "Loss 17393.22543144226\n",
            "Loss 17436.163477897644\n",
            "Loss 17476.72034740448\n",
            "Step 1560, Loss 4.480195045471191\n",
            "Loss 17521.522297859192\n",
            "Loss 17564.816966056824\n",
            "Loss 17611.120467185974\n",
            "Loss 17657.79121875763\n",
            "Loss 17700.81157207489\n",
            "Loss 17744.178686141968\n",
            "Loss 17787.953553199768\n",
            "Loss 17832.451128959656\n",
            "Loss 17876.737942695618\n",
            "Loss 17919.856181144714\n",
            "Step 1570, Loss 4.319863319396973\n",
            "Loss 17963.054814338684\n",
            "Loss 18007.987270355225\n",
            "Loss 18051.35763168335\n",
            "Loss 18095.963978767395\n",
            "Loss 18138.467755317688\n",
            "Loss 18185.614275932312\n",
            "Loss 18227.08300590515\n",
            "Loss 18273.05419445038\n",
            "Loss 18311.02989912033\n",
            "Loss 18356.60489797592\n",
            "Step 1580, Loss 4.395219802856445\n",
            "Loss 18400.557096004486\n",
            "Loss 18445.47714948654\n",
            "Loss 18488.211348056793\n",
            "Loss 18528.28307390213\n",
            "Loss 18571.825306415558\n",
            "Loss 18616.133725643158\n",
            "Loss 18661.62985086441\n",
            "Loss 18704.728438854218\n",
            "Loss 18750.513031482697\n",
            "Loss 18791.202895641327\n",
            "Step 1590, Loss 4.267153739929199\n",
            "Loss 18833.87443304062\n",
            "Loss 18873.13707113266\n",
            "Loss 18915.525834560394\n",
            "Loss 18960.803673267365\n",
            "Loss 19005.71727991104\n",
            "Loss 19047.35699892044\n",
            "Loss 19091.643006801605\n",
            "Loss 19135.556638240814\n",
            "Loss 19179.267070293427\n",
            "Loss 19226.318247318268\n",
            "Step 1600, Loss 4.5774030685424805\n",
            "Loss 19272.092278003693\n",
            "Loss 19316.143171787262\n",
            "Loss 19358.95699739456\n",
            "Loss 19402.21102952957\n",
            "Loss 19444.44261789322\n",
            "Loss 19484.81326341629\n",
            "Loss 19531.207253932953\n",
            "Loss 19573.952252864838\n",
            "Loss 19616.754887104034\n",
            "Loss 19659.206149578094\n",
            "Step 1610, Loss 4.506216526031494\n",
            "Loss 19704.26831483841\n",
            "Loss 19746.94307565689\n",
            "Loss 19788.191339969635\n",
            "Loss 19824.83292579651\n",
            "Loss 19870.989456176758\n",
            "Loss 19909.91164445877\n",
            "Loss 19955.88215112686\n",
            "Loss 20002.26495027542\n",
            "Loss 20046.59898042679\n",
            "Loss 20091.526458263397\n",
            "Step 1620, Loss 4.614899158477783\n",
            "Loss 20137.675449848175\n",
            "Loss 20181.38995885849\n",
            "Loss 20224.59824323654\n",
            "Loss 20266.421744823456\n",
            "Loss 20308.052270412445\n",
            "Loss 20351.86733007431\n",
            "Loss 20398.830420970917\n",
            "Loss 20443.610084056854\n",
            "Loss 20486.868226528168\n",
            "Loss 20532.019588947296\n",
            "Step 1630, Loss 4.582747459411621\n",
            "Loss 20577.847063541412\n",
            "Loss 20621.633636951447\n",
            "Loss 20662.430288791656\n",
            "Loss 20705.780522823334\n",
            "Loss 20751.140296459198\n",
            "Loss 20792.900178432465\n",
            "Loss 20836.610910892487\n",
            "Loss 20881.17906332016\n",
            "Loss 20921.46229982376\n",
            "Loss 20966.477925777435\n",
            "Step 1640, Loss 4.397374153137207\n",
            "Loss 21010.451667308807\n",
            "Loss 21053.591616153717\n",
            "Loss 21099.062931537628\n",
            "Loss 21141.36783838272\n",
            "Loss 21186.042969226837\n",
            "Loss 21230.434205532074\n",
            "Loss 21272.944424152374\n",
            "Loss 21316.15579843521\n",
            "Loss 21358.0713057518\n",
            "Loss 21401.417367458344\n",
            "Step 1650, Loss 4.405518531799316\n",
            "Loss 21445.472552776337\n",
            "Loss 21487.962090969086\n",
            "Loss 21529.503734111786\n",
            "Loss 21573.70540380478\n",
            "Loss 21615.82093000412\n",
            "Loss 21657.122762203217\n",
            "Loss 21696.176283359528\n",
            "Loss 21736.596958637238\n",
            "Loss 21779.080736637115\n",
            "Loss 21819.931008815765\n",
            "Step 1660, Loss 4.154872894287109\n",
            "Loss 21861.479737758636\n",
            "Loss 21904.505417346954\n",
            "Loss 21949.264886379242\n",
            "Loss 21993.971145153046\n",
            "Loss 22036.404569149017\n",
            "Loss 22075.85735797882\n",
            "Loss 22116.243872642517\n",
            "Loss 22161.980237960815\n",
            "Loss 22206.67353630066\n",
            "Loss 22254.30151462555\n",
            "Step 1670, Loss 3.9427497386932373\n",
            "Loss 22293.72901201248\n",
            "Loss 22336.287305355072\n",
            "Loss 22376.013333797455\n",
            "Loss 22420.7305932045\n",
            "Loss 22464.29447412491\n",
            "Loss 22508.4046292305\n",
            "Loss 22555.053408145905\n",
            "Loss 22599.291355609894\n",
            "Loss 22639.974949359894\n",
            "Loss 22683.68111371994\n",
            "Step 1680, Loss 4.485383033752441\n",
            "Loss 22728.534944057465\n",
            "Loss 22773.672511577606\n",
            "Loss 22816.777470111847\n",
            "Loss 22860.575439929962\n",
            "Loss 22896.771228313446\n",
            "Loss 22935.064795017242\n",
            "Loss 22979.622099399567\n",
            "Loss 23020.356867313385\n",
            "Loss 23064.123561382294\n",
            "Loss 23106.446731090546\n",
            "Step 1690, Loss 4.734139919281006\n",
            "Loss 23153.788130283356\n",
            "Loss 23195.85080385208\n",
            "Loss 23242.70478963852\n",
            "Loss 23285.314662456512\n",
            "Loss 23330.176150798798\n",
            "Loss 23369.50496673584\n",
            "Loss 23413.98401737213\n",
            "Loss 23457.986464500427\n",
            "Loss 23499.638743400574\n",
            "Loss 23535.722556114197\n",
            "Step 1700, Loss 4.287393569946289\n",
            "Loss 23578.59649181366\n",
            "Loss 23622.974095344543\n",
            "Loss 23667.68310070038\n",
            "Loss 23709.67875480652\n",
            "Loss 23750.786151885986\n",
            "Loss 23795.568642616272\n",
            "Loss 23839.368681907654\n",
            "Loss 23885.192952156067\n",
            "Loss 23930.619769096375\n",
            "Loss 23971.433696746826\n",
            "Step 1710, Loss 4.050975322723389\n",
            "Loss 24011.94344997406\n",
            "Loss 24058.912572860718\n",
            "Loss 24100.512766838074\n",
            "Loss 24145.92182636261\n",
            "Loss 24186.01674556732\n",
            "Loss 24231.93114757538\n",
            "Loss 24278.506636619568\n",
            "Loss 24322.127532958984\n",
            "Loss 24364.243330955505\n",
            "Loss 24408.591923713684\n",
            "Step 1720, Loss 4.558615207672119\n",
            "Loss 24454.178075790405\n",
            "Loss 24500.33100605011\n",
            "Loss 24542.576460838318\n",
            "Loss 24587.773547172546\n",
            "Loss 24631.40856742859\n",
            "Loss 24671.18052482605\n",
            "Loss 24714.07284259796\n",
            "Loss 24756.465315818787\n",
            "Loss 24800.61237335205\n",
            "Loss 24848.167943954468\n",
            "Step 1730, Loss 4.439654350280762\n",
            "Loss 24892.564487457275\n",
            "Loss 24932.372641563416\n",
            "Loss 24977.63972759247\n",
            "Loss 25017.380793094635\n",
            "\n",
            " Starting epoch 1/30, LR = [0.0001], batch size= 10\n",
            "Loss 42.40767478942871\n",
            "Loss 88.6070966720581\n",
            "Loss 131.8085813522339\n",
            "Loss 171.81395530700684\n",
            "Loss 216.2774658203125\n",
            "Loss 261.64719581604004\n",
            "Step 1740, Loss 4.4082136154174805\n",
            "Loss 305.72933197021484\n",
            "Loss 347.92920112609863\n",
            "Loss 392.7546548843384\n",
            "Loss 431.5579080581665\n",
            "Loss 476.2884569168091\n",
            "Loss 513.9277648925781\n",
            "Loss 557.2447061538696\n",
            "Loss 601.4094734191895\n",
            "Loss 641.7821311950684\n",
            "Loss 682.5891065597534\n",
            "Step 1750, Loss 4.376299858093262\n",
            "Loss 726.352105140686\n",
            "Loss 770.3301334381104\n",
            "Loss 812.1919631958008\n",
            "Loss 853.4944725036621\n",
            "Loss 899.9982357025146\n",
            "Loss 936.1718034744263\n",
            "Loss 980.5987787246704\n",
            "Loss 1022.3842191696167\n",
            "Loss 1063.3312320709229\n",
            "Loss 1104.0739822387695\n",
            "Step 1760, Loss 4.4119696617126465\n",
            "Loss 1148.193678855896\n",
            "Loss 1191.9479417800903\n",
            "Loss 1232.6214838027954\n",
            "Loss 1275.25559425354\n",
            "Loss 1318.938512802124\n",
            "Loss 1362.517147064209\n",
            "Loss 1399.467875957489\n",
            "Loss 1444.8760390281677\n",
            "Loss 1486.5565276145935\n",
            "Loss 1531.5982413291931\n",
            "Step 1770, Loss 4.389834403991699\n",
            "Loss 1575.49658536911\n",
            "Loss 1618.2410979270935\n",
            "Loss 1663.056652545929\n",
            "Loss 1710.3138709068298\n",
            "Loss 1754.4917225837708\n",
            "Loss 1797.7414393424988\n",
            "Loss 1843.2851195335388\n",
            "Loss 1887.8830218315125\n",
            "Loss 1931.6639494895935\n",
            "Loss 1974.3455004692078\n",
            "Step 1780, Loss 4.053683280944824\n",
            "Loss 2014.882333278656\n",
            "Loss 2054.978744983673\n",
            "Loss 2100.4950737953186\n",
            "Loss 2141.9906544685364\n",
            "Loss 2183.9653038978577\n",
            "Loss 2220.533721446991\n",
            "Loss 2263.7491154670715\n",
            "Loss 2307.197325229645\n",
            "Loss 2352.006618976593\n",
            "Loss 2398.9832425117493\n",
            "Step 1790, Loss 4.546971797943115\n",
            "Loss 2444.4529604911804\n",
            "Loss 2485.3650641441345\n",
            "Loss 2523.6174488067627\n",
            "Loss 2566.742491722107\n",
            "Loss 2608.6728286743164\n",
            "Loss 2651.0193014144897\n",
            "Loss 2692.7644443511963\n",
            "Loss 2735.2944707870483\n",
            "Loss 2780.378541946411\n",
            "Loss 2822.05011844635\n",
            "Step 1800, Loss 4.508481025695801\n",
            "Loss 2867.134928703308\n",
            "Loss 2910.54434299469\n",
            "Loss 2954.398856163025\n",
            "Loss 3000.7818174362183\n",
            "Loss 3039.733920097351\n",
            "Loss 3082.475528717041\n",
            "Loss 3128.8534927368164\n",
            "Loss 3168.01726102829\n",
            "Loss 3211.726806163788\n",
            "Loss 3251.410903930664\n",
            "Step 1810, Loss 4.282290935516357\n",
            "Loss 3294.2338132858276\n",
            "Loss 3334.808897972107\n",
            "Loss 3380.286645889282\n",
            "Loss 3419.3134260177612\n",
            "Loss 3462.571988105774\n",
            "Loss 3504.117522239685\n",
            "Loss 3540.286464691162\n",
            "Loss 3584.7959184646606\n",
            "Loss 3624.983425140381\n",
            "Loss 3669.303321838379\n",
            "Step 1820, Loss 3.9678101539611816\n",
            "Loss 3708.9814233779907\n",
            "Loss 3752.4169492721558\n",
            "Loss 3796.9244861602783\n",
            "Loss 3836.675248146057\n",
            "Loss 3879.9353790283203\n",
            "Loss 3927.3601055145264\n",
            "Loss 3970.1349115371704\n",
            "Loss 4010.607180595398\n",
            "Loss 4054.5029306411743\n",
            "Loss 4098.5755252838135\n",
            "Step 1830, Loss 4.3657546043396\n",
            "Loss 4142.2330713272095\n",
            "Loss 4183.630766868591\n",
            "Loss 4228.576231002808\n",
            "Loss 4271.521697044373\n",
            "Loss 4316.924338340759\n",
            "Loss 4363.268399238586\n",
            "Loss 4407.2805643081665\n",
            "Loss 4453.8403606414795\n",
            "Loss 4496.267185211182\n",
            "Loss 4540.739316940308\n",
            "Step 1840, Loss 4.416192531585693\n",
            "Loss 4584.901242256165\n",
            "Loss 4626.365532875061\n",
            "Loss 4670.23012638092\n",
            "Loss 4712.469043731689\n",
            "Loss 4757.271823883057\n",
            "Loss 4798.836817741394\n",
            "Loss 4842.477555274963\n",
            "Loss 4889.353775978088\n",
            "Loss 4932.679862976074\n",
            "Loss 4979.1282081604\n",
            "Step 1850, Loss 4.240365028381348\n",
            "Loss 5021.531858444214\n",
            "Loss 5065.437965393066\n",
            "Loss 5109.821743965149\n",
            "Loss 5149.972777366638\n",
            "Loss 5196.54079914093\n",
            "Loss 5243.184952735901\n",
            "Loss 5282.844090461731\n",
            "Loss 5319.228203296661\n",
            "Loss 5365.074656009674\n",
            "Loss 5407.248656749725\n",
            "Step 1860, Loss 4.331279754638672\n",
            "Loss 5450.561454296112\n",
            "Loss 5494.661362171173\n",
            "Loss 5539.941284656525\n",
            "Loss 5581.707060337067\n",
            "Loss 5623.6331152915955\n",
            "Loss 5667.81893491745\n",
            "Loss 5714.680669307709\n",
            "Loss 5759.4454646110535\n",
            "Loss 5802.833735942841\n",
            "Loss 5844.7029757499695\n",
            "Step 1870, Loss 3.83880352973938\n",
            "Loss 5883.091011047363\n",
            "Loss 5923.235788345337\n",
            "Loss 5964.460048675537\n",
            "Loss 6007.815036773682\n",
            "Loss 6054.106950759888\n",
            "Loss 6099.56205368042\n",
            "Loss 6143.749303817749\n",
            "Loss 6185.799016952515\n",
            "Loss 6230.486660003662\n",
            "Loss 6272.019357681274\n",
            "Step 1880, Loss 4.29631233215332\n",
            "Loss 6314.982481002808\n",
            "Loss 6358.184404373169\n",
            "Loss 6403.743257522583\n",
            "Loss 6444.7642993927\n",
            "Loss 6486.060519218445\n",
            "Loss 6528.475885391235\n",
            "Loss 6568.787269592285\n",
            "Loss 6610.812439918518\n",
            "Loss 6653.9247369766235\n",
            "Loss 6698.096570968628\n",
            "Step 1890, Loss 4.588366985321045\n",
            "Loss 6743.980240821838\n",
            "Loss 6789.740371704102\n",
            "Loss 6833.489785194397\n",
            "Loss 6878.231353759766\n",
            "Loss 6919.674911499023\n",
            "Loss 6964.399194717407\n",
            "Loss 7008.351430892944\n",
            "Loss 7051.977314949036\n",
            "Loss 7094.542064666748\n",
            "Loss 7138.19643497467\n",
            "Step 1900, Loss 4.344091892242432\n",
            "Loss 7181.637353897095\n",
            "Loss 7226.014747619629\n",
            "Loss 7269.7725439071655\n",
            "Loss 7316.969442367554\n",
            "Loss 7358.31693649292\n",
            "Loss 7402.4413776397705\n",
            "Loss 7444.288520812988\n",
            "Loss 7491.3491678237915\n",
            "Loss 7534.384379386902\n",
            "Loss 7578.618068695068\n",
            "Step 1910, Loss 4.409036636352539\n",
            "Loss 7622.708435058594\n",
            "Loss 7666.00604057312\n",
            "Loss 7711.859693527222\n",
            "Loss 7757.006802558899\n",
            "Loss 7800.609097480774\n",
            "Loss 7845.6179094314575\n",
            "Loss 7887.787704467773\n",
            "Loss 7929.6985912323\n",
            "Loss 7976.158604621887\n",
            "Loss 8016.846027374268\n",
            "Step 1920, Loss 4.27146577835083\n",
            "Loss 8059.560685157776\n",
            "Loss 8101.799674034119\n",
            "Loss 8143.049569129944\n",
            "Loss 8183.703923225403\n",
            "Loss 8226.475806236267\n",
            "Loss 8272.434072494507\n",
            "Loss 8317.238955497742\n",
            "Loss 8363.333048820496\n",
            "Loss 8407.131724357605\n",
            "Loss 8446.98816537857\n",
            "Step 1930, Loss 4.499242782592773\n",
            "Loss 8491.980593204498\n",
            "Loss 8538.93338918686\n",
            "Loss 8583.511817455292\n",
            "Loss 8628.791673183441\n",
            "Loss 8672.528660297394\n",
            "Loss 8717.809870243073\n",
            "Loss 8758.161318302155\n",
            "Loss 8803.79721403122\n",
            "Loss 8843.447246551514\n",
            "Loss 8885.866770744324\n",
            "Step 1940, Loss 4.0467963218688965\n",
            "Loss 8926.334733963013\n",
            "Loss 8971.624703407288\n",
            "Loss 9016.298365592957\n",
            "Loss 9059.22236919403\n",
            "Loss 9101.671032905579\n",
            "Loss 9146.598372459412\n",
            "Loss 9188.632235527039\n",
            "Loss 9231.748476028442\n",
            "Loss 9274.90746974945\n",
            "Loss 9320.085182189941\n",
            "Step 1950, Loss 4.251076698303223\n",
            "Loss 9362.595949172974\n",
            "Loss 9408.639245033264\n",
            "Loss 9453.282046318054\n",
            "Loss 9496.341080665588\n",
            "Loss 9537.538261413574\n",
            "Loss 9581.209406852722\n",
            "Loss 9624.91985797882\n",
            "Loss 9666.645221710205\n",
            "Loss 9713.236842155457\n",
            "Loss 9758.321976661682\n",
            "Step 1960, Loss 4.669931888580322\n",
            "Loss 9805.021295547485\n",
            "Loss 9845.631399154663\n",
            "Loss 9888.253598213196\n",
            "Loss 9932.17017173767\n",
            "Loss 9976.18055343628\n",
            "Loss 10025.035181045532\n",
            "Loss 10069.165964126587\n",
            "Loss 10112.956109046936\n",
            "Loss 10157.568998336792\n",
            "Loss 10202.132906913757\n",
            "Step 1970, Loss 4.261547088623047\n",
            "Loss 10244.748377799988\n",
            "Loss 10289.325680732727\n",
            "Loss 10331.590790748596\n",
            "Loss 10371.748566627502\n",
            "Loss 10416.646399497986\n",
            "Loss 10461.511540412903\n",
            "Loss 10504.98302936554\n",
            "Loss 10549.746012687683\n",
            "Loss 10590.139331817627\n",
            "Loss 10633.152713775635\n",
            "Step 1980, Loss 4.096699237823486\n",
            "Loss 10674.11970615387\n",
            "Loss 10718.436541557312\n",
            "Loss 10758.988013267517\n",
            "Loss 10803.932948112488\n",
            "Loss 10846.137652397156\n",
            "Loss 10891.865181922913\n",
            "Loss 10931.455163955688\n",
            "Loss 10976.710357666016\n",
            "Loss 11020.471229553223\n",
            "Loss 11063.11526298523\n",
            "Step 1990, Loss 4.368059158325195\n",
            "Loss 11106.795854568481\n",
            "Loss 11154.327178001404\n",
            "Loss 11197.90337562561\n",
            "Loss 11242.46603012085\n",
            "Loss 11280.97382068634\n",
            "Loss 11325.737133026123\n",
            "Loss 11372.814416885376\n",
            "Loss 11414.69356060028\n",
            "Loss 11456.638779640198\n",
            "Loss 11498.469185829163\n",
            "Step 2000, Loss 4.536797046661377\n",
            "Loss 11543.837156295776\n",
            "Loss 11584.994101524353\n",
            "Loss 11626.641373634338\n",
            "Loss 11671.290717124939\n",
            "Loss 11714.292511940002\n",
            "Loss 11758.868627548218\n",
            "Loss 11806.911735534668\n",
            "Loss 11847.822589874268\n",
            "Loss 11894.545154571533\n",
            "Loss 11935.121474266052\n",
            "Step 2010, Loss 4.270392417907715\n",
            "Loss 11977.82539844513\n",
            "Loss 12023.02297115326\n",
            "Loss 12065.6898355484\n",
            "Loss 12105.943059921265\n",
            "Loss 12151.885471343994\n",
            "Loss 12198.73906135559\n",
            "Loss 12238.962421417236\n",
            "Loss 12284.28376197815\n",
            "Loss 12329.071063995361\n",
            "Loss 12375.392279624939\n",
            "Step 2020, Loss 4.20365047454834\n",
            "Loss 12417.428784370422\n",
            "Loss 12460.085034370422\n",
            "Loss 12503.532190322876\n",
            "Loss 12543.441557884216\n",
            "Loss 12583.894233703613\n",
            "Loss 12625.884990692139\n",
            "Loss 12667.8768825531\n",
            "Loss 12710.437970161438\n",
            "Loss 12755.459685325623\n",
            "Loss 12797.142643928528\n",
            "Step 2030, Loss 4.225071907043457\n",
            "Loss 12839.393362998962\n",
            "Loss 12880.465369224548\n",
            "Loss 12926.817622184753\n",
            "Loss 12969.88474369049\n",
            "Loss 13014.701361656189\n",
            "Loss 13058.525857925415\n",
            "Loss 13101.21500492096\n",
            "Loss 13142.416434288025\n",
            "Loss 13184.924216270447\n",
            "Loss 13227.067589759827\n",
            "Step 2040, Loss 4.666014194488525\n",
            "Loss 13273.727731704712\n",
            "Loss 13315.109882354736\n",
            "Loss 13357.615246772766\n",
            "Loss 13403.283009529114\n",
            "Loss 13446.627025604248\n",
            "Loss 13493.172154426575\n",
            "Loss 13536.927280426025\n",
            "Loss 13578.388242721558\n",
            "Loss 13622.318396568298\n",
            "Loss 13670.087509155273\n",
            "Step 2050, Loss 4.028380393981934\n",
            "Loss 13710.371313095093\n",
            "Loss 13751.747326850891\n",
            "Loss 13795.507102012634\n",
            "Loss 13835.551705360413\n",
            "Loss 13879.518132209778\n",
            "Loss 13924.251971244812\n",
            "Loss 13962.172961235046\n",
            "Loss 14004.227743148804\n",
            "Loss 14048.043718338013\n",
            "Loss 14091.349864006042\n",
            "Step 2060, Loss 4.643451690673828\n",
            "Loss 14137.78438091278\n",
            "Loss 14185.176801681519\n",
            "Loss 14228.500709533691\n",
            "Loss 14273.5280752182\n",
            "Loss 14313.558373451233\n",
            "Loss 14358.18890094757\n",
            "Loss 14400.881142616272\n",
            "Loss 14445.083646774292\n",
            "Loss 14491.031436920166\n",
            "Loss 14531.841568946838\n",
            "Step 2070, Loss 4.005255699157715\n",
            "Loss 14571.894125938416\n",
            "Loss 14617.48821258545\n",
            "Loss 14661.192378997803\n",
            "Loss 14705.233383178711\n",
            "Loss 14750.67195892334\n",
            "Loss 14791.723051071167\n",
            "Loss 14834.50204372406\n",
            "Loss 14877.83329486847\n",
            "Loss 14920.30800819397\n",
            "Loss 14962.408547401428\n",
            "Step 2080, Loss 4.125735759735107\n",
            "Loss 15003.66590499878\n",
            "Loss 15045.138711929321\n",
            "Loss 15091.040434837341\n",
            "Loss 15134.661016464233\n",
            "Loss 15180.12617111206\n",
            "Loss 15224.828252792358\n",
            "Loss 15270.220427513123\n",
            "Loss 15316.404733657837\n",
            "Loss 15361.157302856445\n",
            "Loss 15404.114351272583\n",
            "Step 2090, Loss 4.492636203765869\n",
            "Loss 15449.040713310242\n",
            "Loss 15492.567591667175\n",
            "Loss 15534.823694229126\n",
            "Loss 15574.24211025238\n",
            "Loss 15620.109648704529\n",
            "Loss 15666.215648651123\n",
            "Loss 15706.795258522034\n",
            "Loss 15750.029745101929\n",
            "Loss 15795.09346485138\n",
            "Loss 15836.67354106903\n",
            "Step 2100, Loss 4.418628215789795\n",
            "Loss 15880.859823226929\n",
            "Loss 15920.9800863266\n",
            "Loss 15963.688564300537\n",
            "Loss 16010.905284881592\n",
            "Loss 16055.116243362427\n",
            "Loss 16098.518671989441\n",
            "Loss 16143.32965373993\n",
            "Loss 16181.705031394958\n",
            "Loss 16224.110674858093\n",
            "Loss 16269.454426765442\n",
            "Step 2110, Loss 4.148785591125488\n",
            "Loss 16310.942282676697\n",
            "Loss 16353.855361938477\n",
            "Loss 16398.442358970642\n",
            "Loss 16436.265420913696\n",
            "Loss 16481.3104057312\n",
            "Loss 16520.342166423798\n",
            "Loss 16565.59378862381\n",
            "Loss 16606.80545091629\n",
            "Loss 16652.655670642853\n",
            "Loss 16697.41809606552\n",
            "Step 2120, Loss 4.2988691329956055\n",
            "Loss 16740.406787395477\n",
            "Loss 16783.756959438324\n",
            "Loss 16828.411123752594\n",
            "Loss 16874.08101797104\n",
            "Loss 16916.4963889122\n",
            "Loss 16957.584273815155\n",
            "Loss 17005.366933345795\n",
            "Loss 17047.1981549263\n",
            "Loss 17093.313672542572\n",
            "Loss 17139.758269786835\n",
            "Step 2130, Loss 4.521084785461426\n",
            "Loss 17184.96911764145\n",
            "Loss 17229.15076494217\n",
            "Loss 17272.854931354523\n",
            "Loss 17319.648134708405\n",
            "Loss 17362.826025485992\n",
            "Loss 17404.487545490265\n",
            "Loss 17446.42160654068\n",
            "Loss 17490.71972131729\n",
            "Loss 17530.780589580536\n",
            "Loss 17577.990844249725\n",
            "Step 2140, Loss 4.188725471496582\n",
            "Loss 17619.87809896469\n",
            "Loss 17662.680633068085\n",
            "Loss 17705.99369764328\n",
            "Loss 17744.36839580536\n",
            "Loss 17786.72797203064\n",
            "Loss 17831.019892692566\n",
            "Loss 17871.26808643341\n",
            "Loss 17914.84923362732\n",
            "Loss 17959.364023208618\n",
            "Loss 18001.384167671204\n",
            "Step 2150, Loss 4.181246280670166\n",
            "Loss 18043.196630477905\n",
            "Loss 18086.027183532715\n",
            "Loss 18129.73738670349\n",
            "Loss 18171.811065673828\n",
            "Loss 18214.971342086792\n",
            "Loss 18255.56917667389\n",
            "Loss 18297.459502220154\n",
            "Loss 18341.17630958557\n",
            "Loss 18385.964159965515\n",
            "Loss 18424.802832603455\n",
            "Step 2160, Loss 4.516770362854004\n",
            "Loss 18469.970536231995\n",
            "Loss 18515.94096183777\n",
            "Loss 18558.287410736084\n",
            "Loss 18605.544896125793\n",
            "Loss 18649.59668636322\n",
            "Loss 18691.137342453003\n",
            "Loss 18731.778087615967\n",
            "Loss 18773.15507888794\n",
            "Loss 18816.788187026978\n",
            "Loss 18860.44852733612\n",
            "Step 2170, Loss 4.390194892883301\n",
            "Loss 18904.350476264954\n",
            "Loss 18948.26014995575\n",
            "Loss 18993.609371185303\n",
            "Loss 19034.047260284424\n",
            "Loss 19079.185547828674\n",
            "Loss 19121.638927459717\n",
            "Loss 19166.352438926697\n",
            "Loss 19210.472569465637\n",
            "Loss 19251.408381462097\n",
            "Loss 19295.11423587799\n",
            "Step 2180, Loss 4.387711524963379\n",
            "Loss 19338.991351127625\n",
            "Loss 19381.122221946716\n",
            "Loss 19420.529177188873\n",
            "Loss 19462.538673877716\n",
            "Loss 19505.762450695038\n",
            "Loss 19545.877883434296\n",
            "Loss 19589.285695552826\n",
            "Loss 19631.203095912933\n",
            "Loss 19675.801594257355\n",
            "Loss 19719.823348522186\n",
            "Step 2190, Loss 4.531834125518799\n",
            "Loss 19765.141689777374\n",
            "Loss 19806.449143886566\n",
            "Loss 19851.46453142166\n",
            "Loss 19897.568628787994\n",
            "Loss 19944.570858478546\n",
            "Loss 19989.900772571564\n",
            "Loss 20032.56623029709\n",
            "Loss 20075.51707983017\n",
            "Loss 20118.174040317535\n",
            "Loss 20161.500160694122\n",
            "Step 2200, Loss 3.4474480152130127\n",
            "Loss 20195.974640846252\n",
            "Loss 20242.568130493164\n",
            "Loss 20286.458582878113\n",
            "Loss 20331.599469184875\n",
            "Loss 20375.267252922058\n",
            "Loss 20415.87656021118\n",
            "Loss 20458.86399269104\n",
            "Loss 20499.699025154114\n",
            "Loss 20544.46261882782\n",
            "Loss 20592.549114227295\n",
            "Step 2210, Loss 4.481466770172119\n",
            "Loss 20637.363781929016\n",
            "Loss 20679.7278547287\n",
            "Loss 20723.609347343445\n",
            "Loss 20767.857341766357\n",
            "Loss 20809.92651939392\n",
            "Loss 20853.937616348267\n",
            "Loss 20889.883790016174\n",
            "Loss 20932.784876823425\n",
            "Loss 20978.811988830566\n",
            "Loss 21024.31360721588\n",
            "Step 2220, Loss 4.591336727142334\n",
            "Loss 21070.226974487305\n",
            "Loss 21112.275018692017\n",
            "Loss 21157.97203063965\n",
            "Loss 21197.17216014862\n",
            "Loss 21239.997692108154\n",
            "Loss 21283.945298194885\n",
            "Loss 21328.615698814392\n",
            "Loss 21372.856130599976\n",
            "Loss 21416.150908470154\n",
            "Loss 21461.10683441162\n",
            "Step 2230, Loss 4.605368614196777\n",
            "Loss 21507.16052055359\n",
            "Loss 21549.554624557495\n",
            "Loss 21589.626927375793\n",
            "Loss 21633.47146987915\n",
            "Loss 21676.331734657288\n",
            "Loss 21716.55583381653\n",
            "Loss 21760.12246608734\n",
            "Loss 21803.781542778015\n",
            "Loss 21844.303164482117\n",
            "Loss 21888.837761878967\n",
            "Step 2240, Loss 4.058438777923584\n",
            "Loss 21929.422149658203\n",
            "Loss 21975.127382278442\n",
            "Loss 22018.998775482178\n",
            "Loss 22061.226148605347\n",
            "Loss 22101.262383461\n",
            "Loss 22144.43664073944\n",
            "Loss 22187.935967445374\n",
            "Loss 22230.635981559753\n",
            "Loss 22273.46423149109\n",
            "Loss 22314.817867279053\n",
            "Step 2250, Loss 4.319913864135742\n",
            "Loss 22358.01700592041\n",
            "Loss 22399.729890823364\n",
            "Loss 22446.388940811157\n",
            "Loss 22489.591584205627\n",
            "Loss 22534.15572166443\n",
            "Loss 22573.810420036316\n",
            "Loss 22616.202549934387\n",
            "Loss 22659.166598320007\n",
            "Loss 22700.465626716614\n",
            "Loss 22745.002455711365\n",
            "Step 2260, Loss 4.408636093139648\n",
            "Loss 22789.08881664276\n",
            "Loss 22832.610750198364\n",
            "Loss 22876.41640663147\n",
            "Loss 22917.089762687683\n",
            "Loss 22960.30635356903\n",
            "Loss 23000.803356170654\n",
            "Loss 23041.711673736572\n",
            "Loss 23085.038328170776\n",
            "Loss 23128.857069015503\n",
            "Loss 23176.67547225952\n",
            "Step 2270, Loss 4.270542621612549\n",
            "Loss 23219.380898475647\n",
            "Loss 23260.949754714966\n",
            "Loss 23306.95086479187\n",
            "Loss 23350.646905899048\n",
            "Loss 23395.07408618927\n",
            "Loss 23439.16961669922\n",
            "Loss 23482.91880607605\n",
            "Loss 23528.54245185852\n",
            "Loss 23573.099007606506\n",
            "Loss 23616.267948150635\n",
            "Step 2280, Loss 4.7087225914001465\n",
            "Loss 23663.355174064636\n",
            "Loss 23706.720280647278\n",
            "Loss 23753.833827972412\n",
            "Loss 23796.794118881226\n",
            "Loss 23844.51072692871\n",
            "Loss 23887.17848777771\n",
            "Loss 23931.10092163086\n",
            "Loss 23974.94767189026\n",
            "Loss 24020.591597557068\n",
            "Loss 24060.53300857544\n",
            "Step 2290, Loss 4.411042213439941\n",
            "Loss 24104.64343070984\n",
            "Loss 24144.901719093323\n",
            "Loss 24187.968640327454\n",
            "Loss 24232.935333251953\n",
            "Loss 24277.190399169922\n",
            "Loss 24320.345797538757\n",
            "Loss 24365.61631679535\n",
            "Loss 24408.884692192078\n",
            "Loss 24454.674463272095\n",
            "Loss 24500.29550552368\n",
            "Step 2300, Loss 3.9392647743225098\n",
            "Loss 24539.688153266907\n",
            "Loss 24582.95774459839\n",
            "Loss 24626.154794692993\n",
            "Loss 24666.298022270203\n",
            "Loss 24711.592783927917\n",
            "Loss 24753.964715003967\n",
            "Loss 24795.99594116211\n",
            "Loss 24838.46634864807\n",
            "Loss 24882.608890533447\n",
            "Loss 24926.211366653442\n",
            "Step 2310, Loss 4.002426624298096\n",
            "Loss 24966.235632896423\n",
            "Loss 25008.956751823425\n",
            "\n",
            " Starting epoch 1/30, LR = [0.1], batch size= 100\n",
            "Loss 426.15723609924316\n",
            "Loss 859.7044944763184\n",
            "Loss 1295.1590061187744\n",
            "Loss 1701.3773441314697\n",
            "Loss 2102.870512008667\n",
            "Loss 2564.864730834961\n",
            "Loss 2999.9444007873535\n",
            "Loss 3417.455530166626\n",
            "Step 2320, Loss 4.347842216491699\n",
            "Loss 3852.239751815796\n",
            "Loss 4272.689962387085\n",
            "Loss 4687.9321575164795\n",
            "Loss 5126.349258422852\n",
            "Loss 5560.3991985321045\n",
            "Loss 5959.1625690460205\n",
            "Loss 6388.780784606934\n",
            "Loss 6814.664793014526\n",
            "Loss 7239.584398269653\n",
            "Loss 7659.566354751587\n",
            "Step 2330, Loss 4.387282848358154\n",
            "Loss 8098.294639587402\n",
            "Loss 8512.115001678467\n",
            "Loss 8931.497430801392\n",
            "Loss 9347.044944763184\n",
            "Loss 9773.126697540283\n",
            "Loss 10215.802097320557\n",
            "Loss 10632.888269424438\n",
            "Loss 11058.647775650024\n",
            "Loss 11485.731410980225\n",
            "Loss 11921.603918075562\n",
            "Step 2340, Loss 4.1148295402526855\n",
            "Loss 12333.08687210083\n",
            "Loss 12754.27279472351\n",
            "Loss 13171.349096298218\n",
            "Loss 13592.109251022339\n",
            "Loss 14016.252756118774\n",
            "Loss 14450.067567825317\n",
            "Loss 14879.851198196411\n",
            "Loss 15301.975059509277\n",
            "Loss 15704.437971115112\n",
            "Loss 16114.206790924072\n",
            "Step 2350, Loss 4.443336009979248\n",
            "Loss 16558.540391921997\n",
            "Loss 16986.651372909546\n",
            "Loss 17408.27989578247\n",
            "Loss 17831.845378875732\n",
            "Loss 18266.27712249756\n",
            "Loss 18692.749643325806\n",
            "Loss 19104.774618148804\n",
            "Loss 19540.227270126343\n",
            "Loss 19970.78604698181\n",
            "Loss 20398.604822158813\n",
            "Step 2360, Loss 4.2970805168151855\n",
            "Loss 20828.312873840332\n",
            "Loss 21266.381216049194\n",
            "Loss 21677.089548110962\n",
            "Loss 22087.284803390503\n",
            "Loss 22491.649436950684\n",
            "Loss 22927.221727371216\n",
            "Loss 23358.107328414917\n",
            "Loss 23778.42254638672\n",
            "Loss 24192.505931854248\n",
            "\n",
            " Starting epoch 1/30, LR = [0.01], batch size= 100\n",
            "Loss 408.32223892211914\n",
            "Step 2370, Loss 4.119661331176758\n",
            "Loss 820.2883720397949\n",
            "Loss 1237.694263458252\n",
            "Loss 1669.749116897583\n",
            "Loss 2090.608501434326\n",
            "Loss 2521.4762687683105\n",
            "Loss 2930.2464962005615\n",
            "Loss 3358.940839767456\n",
            "Loss 3778.4298419952393\n",
            "Loss 4192.042827606201\n",
            "Loss 4621.388483047485\n",
            "Step 2380, Loss 4.2239251136779785\n",
            "Loss 5043.780994415283\n",
            "Loss 5457.755565643311\n",
            "Loss 5869.572877883911\n",
            "Loss 6300.094652175903\n",
            "Loss 6734.219264984131\n",
            "Loss 7152.6209354400635\n",
            "Loss 7573.091650009155\n",
            "Loss 7979.393625259399\n",
            "Loss 8415.34514427185\n",
            "Loss 8843.373680114746\n",
            "Step 2390, Loss 4.01077938079834\n",
            "Loss 9244.45161819458\n",
            "Loss 9646.271324157715\n",
            "Loss 10068.192100524902\n",
            "Loss 10480.529832839966\n",
            "Loss 10882.802486419678\n",
            "Loss 11287.201261520386\n",
            "Loss 11713.910722732544\n",
            "Loss 12113.61174583435\n",
            "Loss 12516.426944732666\n",
            "Loss 12929.771900177002\n",
            "Step 2400, Loss 4.179356575012207\n",
            "Loss 13347.707557678223\n",
            "Loss 13763.864946365356\n",
            "Loss 14195.76382637024\n",
            "Loss 14601.324701309204\n",
            "Loss 15027.964448928833\n",
            "Loss 15470.385217666626\n",
            "Loss 15887.975263595581\n",
            "Loss 16307.574701309204\n",
            "Loss 16738.174867630005\n",
            "Loss 17155.855894088745\n",
            "Step 2410, Loss 4.211399555206299\n",
            "Loss 17576.995849609375\n",
            "Loss 17990.866231918335\n",
            "Loss 18415.652322769165\n",
            "Loss 18831.25581741333\n",
            "Loss 19255.142784118652\n",
            "Loss 19668.462419509888\n",
            "Loss 20106.192541122437\n",
            "Loss 20527.280855178833\n",
            "Loss 20967.36650466919\n",
            "Loss 21388.412952423096\n",
            "Step 2420, Loss 4.234707355499268\n",
            "Loss 21811.883687973022\n",
            "Loss 22242.599153518677\n",
            "Loss 22667.79475212097\n",
            "Loss 23083.804750442505\n",
            "Loss 23507.718181610107\n",
            "Loss 23917.705631256104\n",
            "\n",
            " Starting epoch 1/30, LR = [0.001], batch size= 100\n",
            "Loss 417.5440788269043\n",
            "Loss 847.6253986358643\n",
            "Loss 1274.8700141906738\n",
            "Loss 1681.2767028808594\n",
            "Step 2430, Loss 4.076154708862305\n",
            "Loss 2088.89217376709\n",
            "Loss 2524.666690826416\n",
            "Loss 2953.761672973633\n",
            "Loss 3365.4520511627197\n",
            "Loss 3782.033157348633\n",
            "Loss 4217.294979095459\n",
            "Loss 4621.273326873779\n",
            "Loss 5024.822425842285\n",
            "Loss 5444.342374801636\n",
            "Loss 5872.106122970581\n",
            "Step 2440, Loss 4.186568737030029\n",
            "Loss 6290.762996673584\n",
            "Loss 6692.058563232422\n",
            "Loss 7111.170816421509\n",
            "Loss 7517.09098815918\n",
            "Loss 7942.438745498657\n",
            "Loss 8356.90655708313\n",
            "Loss 8771.396017074585\n",
            "Loss 9206.096839904785\n",
            "Loss 9618.674230575562\n",
            "Loss 10020.462131500244\n",
            "Step 2450, Loss 4.185678482055664\n",
            "Loss 10439.02997970581\n",
            "Loss 10848.055696487427\n",
            "Loss 11271.451711654663\n",
            "Loss 11699.702644348145\n",
            "Loss 12097.156047821045\n",
            "Loss 12516.898679733276\n",
            "Loss 12945.103073120117\n",
            "Loss 13360.260343551636\n",
            "Loss 13775.864505767822\n",
            "Loss 14196.95873260498\n",
            "Step 2460, Loss 4.287715911865234\n",
            "Loss 14625.730323791504\n",
            "Loss 15055.182790756226\n",
            "Loss 15480.33537864685\n",
            "Loss 15890.880012512207\n",
            "Loss 16319.086217880249\n",
            "Loss 16725.407218933105\n",
            "Loss 17135.38417816162\n",
            "Loss 17548.334169387817\n",
            "Loss 17964.201736450195\n",
            "Loss 18395.786952972412\n",
            "Step 2470, Loss 4.3637285232543945\n",
            "Loss 18832.15980529785\n",
            "Loss 19243.667793273926\n",
            "Loss 19674.69973564148\n",
            "Loss 20106.67152404785\n",
            "Loss 20520.216369628906\n",
            "Loss 20931.348705291748\n",
            "Loss 21347.97372817993\n",
            "Loss 21767.747497558594\n",
            "Loss 22184.013986587524\n",
            "Loss 22616.886377334595\n",
            "Step 2480, Loss 4.367728233337402\n",
            "Loss 23053.659200668335\n",
            "Loss 23454.790353775024\n",
            "Loss 23872.63641357422\n",
            "\n",
            " Starting epoch 1/30, LR = [0.0001], batch size= 100\n",
            "Loss 428.9278507232666\n",
            "Loss 846.7557907104492\n",
            "Loss 1265.9367561340332\n",
            "Loss 1679.4058322906494\n",
            "Loss 2104.6566486358643\n",
            "Loss 2524.725580215454\n",
            "Loss 2958.2053184509277\n",
            "Step 2490, Loss 4.072957992553711\n",
            "Loss 3365.501117706299\n",
            "Loss 3806.6303730010986\n",
            "Loss 4222.923517227173\n",
            "Loss 4659.567213058472\n",
            "Loss 5098.165607452393\n",
            "Loss 5518.001413345337\n",
            "Loss 5925.224494934082\n",
            "Loss 6334.526777267456\n",
            "Loss 6759.390211105347\n",
            "Loss 7188.767910003662\n",
            "Step 2500, Loss 4.141201019287109\n",
            "Loss 7602.888011932373\n",
            "Loss 8026.3365268707275\n",
            "Loss 8446.004819869995\n",
            "Loss 8854.422187805176\n",
            "Loss 9252.359867095947\n",
            "Loss 9670.107746124268\n",
            "Loss 10099.886274337769\n",
            "Loss 10511.44814491272\n",
            "Loss 10934.549045562744\n",
            "Loss 11356.12587928772\n",
            "Step 2510, Loss 4.159650802612305\n",
            "Loss 11772.09095954895\n",
            "Loss 12196.155881881714\n",
            "Loss 12631.87165260315\n",
            "Loss 13047.559356689453\n",
            "Loss 13435.039830207825\n",
            "Loss 13860.143208503723\n",
            "Loss 14278.24718952179\n",
            "Loss 14693.260884284973\n",
            "Loss 15103.367638587952\n",
            "Loss 15539.398074150085\n",
            "Step 2520, Loss 4.23996114730835\n",
            "Loss 15963.39418888092\n",
            "Loss 16364.074158668518\n",
            "Loss 16785.15441417694\n",
            "Loss 17192.88990497589\n",
            "Loss 17600.75924396515\n",
            "Loss 18016.140055656433\n",
            "Loss 18436.340069770813\n",
            "Loss 18855.033040046692\n",
            "Loss 19271.33400440216\n",
            "Loss 19700.386023521423\n",
            "Step 2530, Loss 4.03805685043335\n",
            "Loss 20104.19170856476\n",
            "Loss 20518.547320365906\n",
            "Loss 20933.30476284027\n",
            "Loss 21360.6467962265\n",
            "Loss 21781.594967842102\n",
            "Loss 22196.131110191345\n",
            "Loss 22619.31927204132\n",
            "Loss 23028.297638893127\n",
            "Loss 23438.917660713196\n",
            "Loss 23864.064049720764\n",
            "\n",
            " Starting epoch 1/30, LR = [0.1], batch size= 1000\n",
            "Step 2540, Loss 4.193946838378906\n",
            "Loss 4193.946838378906\n",
            "Loss 8347.875118255615\n",
            "Loss 12532.07778930664\n",
            "Loss 16723.07825088501\n",
            "Loss 20893.532276153564\n",
            "\n",
            " Starting epoch 1/30, LR = [0.01], batch size= 1000\n",
            "Loss 4260.3254318237305\n",
            "Loss 8440.19079208374\n",
            "Loss 12646.92497253418\n",
            "Loss 16851.13477706909\n",
            "Loss 21056.222438812256\n",
            "\n",
            " Starting epoch 1/30, LR = [0.001], batch size= 1000\n",
            "Step 2550, Loss 4.21004581451416\n",
            "Loss 4210.04581451416\n",
            "Loss 8404.187679290771\n",
            "Loss 12586.655616760254\n",
            "Loss 16823.30274581909\n",
            "Loss 21067.52634048462\n",
            "\n",
            " Starting epoch 1/30, LR = [0.0001], batch size= 1000\n",
            "Loss 4234.058380126953\n",
            "Loss 8493.616580963135\n",
            "Loss 12722.880363464355\n",
            "Loss 16929.19111251831\n",
            "Loss 21137.969970703125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHFI-GAJd69",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO3HV5pqJg1o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "378c3cc8-fe21-4239-cfd6-4e9dd7c91827"
      },
      "source": [
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:13<00:00,  1.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.13861043899066713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
